{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conversational Analysis with GPT-2 Miebaka Worika",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqDLvml4EIIQ",
        "colab_type": "code",
        "outputId": "944ad0c1-c8b1-4939-aff9-31f621f93403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5hMIL5cEhnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from numpy import int64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNYBlLamEoM7",
        "colab_type": "code",
        "outputId": "a2e226a3-d85c-4182-f95a-0fcf64369199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/tenoke/gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdqI8KxOA0UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('gpt-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxB0iPDIGwaN",
        "colab_type": "code",
        "outputId": "1f811dd7-4b0c-4002-d1a1-ee163c9dde1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlX7JBf2NjV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir .kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htekr8W9FC4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7231d117-b85b-4b7c-af85-05082bc0a9d7"
      },
      "source": [
        "ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.gpu     \u001b[0m\u001b[01;34mgpt-2-samples\u001b[0m/  requirements.txt  \u001b[01;32mtrain.py\u001b[0m*\n",
            "DEVELOPERS.md    download_model.py  LICENSE         \u001b[01;34msrc\u001b[0m/\n",
            "Dockerfile.cpu   \u001b[01;32mencode.py\u001b[0m*         README.md       train-horovod.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6fGuRJnE7vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token = {\"username\":\"xianshou\",\"key\":\"9df2f9227d52fa331b2ea240826b976f\"}\n",
        "with open('/content/gpt-2/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8iyhQfHVJI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/gpt-2/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2izSZm0Gid4",
        "colab_type": "code",
        "outputId": "228d11a0-058f-409c-ea7b-fbd5deb705b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!kaggle config set -n path -v{/content}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUejg0RhIwx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpXFmJe0I6dH",
        "colab_type": "code",
        "outputId": "c6d0bc62-52c9-4122-f5c0-664e3fb21932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus -p /content/gpt-2 --force"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ubuntu-dialogue-corpus.zip to /content/gpt-2\n",
            " 99% 788M/799M [00:09<00:00, 114MB/s] \n",
            "100% 799M/799M [00:09<00:00, 90.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxqpvnzTKwJ-",
        "colab_type": "code",
        "outputId": "6971248d-74f4-41a0-f5f9-a25531f57a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip ubuntu-dialogue-corpus.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ubuntu-dialogue-corpus.zip\n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n",
            "  inflating: toc.csv                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ymvAOrfMNZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#By and large if we want to use all datasets - uncomment as necessary.\n",
        "#For this exercise I have only used dialogueText_301.csv\n",
        "\n",
        "#dialogue_1 = 'Ubuntu-dialogue-corpus/dialogueText.csv'\n",
        "#dialogue_2 = 'Ubuntu-dialogue-corpus/dialogueText_196.csv'\n",
        "dialogue_3 = 'Ubuntu-dialogue-corpus/dialogueText_301.csv'\n",
        "\n",
        "#d1_df = pd.read_csv(dialogue_1, parse_dates=['date'], chunksize=1200000)\n",
        "#d2_df = pd.read_csv(dialogue_2, parse_dates=['date'], chunksize=1200000)\n",
        "d3_df = pd.read_csv(dialogue_3, parse_dates=['date'], chunksize=1200000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvgV8VZP-Jww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a4ace08d-b012-4ce9-985e-d4a21f1cdef3"
      },
      "source": [
        "!python download_model.py 345M"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 793kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 57.8Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 801kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:22, 62.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.71Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 60.1Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 56.4Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mozBer9eV2N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ubuntu-data ubuntu-npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fE9adNPsYxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function creates a processed version of the csv\n",
        "#extracting the content we need: date, sender, message\n",
        "def clean(dataset, file_name):\n",
        "  i = 1\n",
        "  for data in dataset:    \n",
        "    text_corpus = ''\n",
        "    current = None\n",
        "    for msg in data.itertuples():\n",
        "      if msg.dialogueID != current:\n",
        "        current = msg.dialogueID\n",
        "        text_corpus += '\\n\\n'\n",
        "      try: \n",
        "        text_corpus += f\"({msg.date}) {msg._4}: {msg.text}\\n\"\n",
        "      except KeyError:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    with open(f'ubuntu-data/{file_name}_{i}.txt', 'w') as f:\n",
        "      f.write(text_corpus)  \n",
        "    del(text_corpus)\n",
        "    i += 1\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSCDj4GwMO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We only read the d3_df so we are only cleaning that file\n",
        "#clean(d1_df, 'cleaned_d1_df')\n",
        "#(d2_df, 'cleaned_d2_df')\n",
        "clean(d3_df, 'cleaned_d3_df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-SIG-TbFOcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "a5ccdb41-9dd1-4a65-e12b-90bfee3619da"
      },
      "source": [
        "#Encoding processed files generated from the csv as necessary\n",
        "#to find this file I used the 'ls' command\n",
        "#after changing to the appropriate directory\n",
        " \n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d1_df_1.txt ubuntu-npz/cleaned_d1_df_1.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_1.txt ubuntu-npz/cleaned_d2_df_1.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_2.txt ubuntu-npz/cleaned_d2_df_2.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_3.txt ubuntu-npz/cleaned_d2_df_3.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_4.txt ubuntu-npz/cleaned_d2_df_4.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_5.txt ubuntu-npz/cleaned_d2_df_5.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_6.txt ubuntu-npz/cleaned_d2_df_6.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_7.txt ubuntu-npz/cleaned_d2_df_7.txt.npz --model_name 345M\n",
        "#!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_8.txt ubuntu-npz/cleaned_d2_df_8.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_1.txt ubuntu-npz/cleaned_d3_df_1.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_2.txt ubuntu-npz/cleaned_d3_df_2.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_3.txt ubuntu-npz/cleaned_d3_df_3.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_4.txt ubuntu-npz/cleaned_d3_df_4.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_5.txt ubuntu-npz/cleaned_d3_df_5.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_6.txt ubuntu-npz/cleaned_d3_df_6.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_7.txt ubuntu-npz/cleaned_d3_df_7.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_8.txt ubuntu-npz/cleaned_d3_df_8.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_9.txt ubuntu-npz/cleaned_d3_df_9.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_10.txt ubuntu-npz/cleaned_d3_df_10.txt.npz --model_name 345M"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [03:31<00:00, 211.21s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_1.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:39<00:00, 219.31s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_2.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:32<00:00, 212.86s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_3.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:32<00:00, 212.35s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_4.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:41<00:00, 221.29s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_5.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:54<00:00, 234.07s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_6.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:55<00:00, 235.51s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_7.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:37<00:00, 217.52s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_8.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:40<00:00, 220.32s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_9.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [04:41<00:00, 281.58s/it]\n",
            "Writing ubuntu-npz/cleaned_d3_df_10.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z5L1bZSe_2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "095ea554-a9ea-4525-a3a4-0863ddaba5e6"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --stop_after 1501 --model_name 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 12:03:37.208674: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 12:03:37.210462: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21d9800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 12:03:37.210504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 10/10 [00:12<00:00,  1.28s/it]\n",
            "dataset has 421469705 tokens\n",
            "Training...\n",
            "[1 | 27.71] loss=1.81 avg=1.81\n",
            "[2 | 43.38] loss=1.80 avg=1.81\n",
            "[3 | 59.53] loss=2.17 avg=1.93\n",
            "[4 | 76.14] loss=1.74 avg=1.88\n",
            "[5 | 92.91] loss=1.84 avg=1.87\n",
            "[6 | 110.02] loss=2.01 avg=1.90\n",
            "[7 | 126.52] loss=2.11 avg=1.93\n",
            "[8 | 142.98] loss=2.34 avg=1.98\n",
            "[9 | 159.46] loss=1.56 avg=1.93\n",
            "[10 | 175.96] loss=2.12 avg=1.95\n",
            "[11 | 192.47] loss=1.53 avg=1.91\n",
            "[12 | 208.97] loss=1.55 avg=1.88\n",
            "[13 | 225.39] loss=2.30 avg=1.91\n",
            "[14 | 242.08] loss=1.76 avg=1.90\n",
            "[15 | 258.51] loss=1.82 avg=1.90\n",
            "[16 | 275.26] loss=1.72 avg=1.88\n",
            "[17 | 291.39] loss=1.61 avg=1.87\n",
            "[18 | 307.70] loss=1.94 avg=1.87\n",
            "[19 | 323.80] loss=1.94 avg=1.87\n",
            "[20 | 340.19] loss=1.48 avg=1.85\n",
            "[21 | 356.40] loss=1.94 avg=1.86\n",
            "[22 | 372.40] loss=1.74 avg=1.85\n",
            "[23 | 388.84] loss=1.68 avg=1.84\n",
            "[24 | 405.42] loss=1.67 avg=1.84\n",
            "[25 | 422.84] loss=1.69 avg=1.83\n",
            "[26 | 439.13] loss=1.57 avg=1.82\n",
            "[27 | 455.42] loss=1.78 avg=1.82\n",
            "[28 | 471.86] loss=1.61 avg=1.81\n",
            "[29 | 488.26] loss=1.92 avg=1.81\n",
            "[30 | 504.53] loss=2.03 avg=1.82\n",
            "[31 | 521.02] loss=2.44 avg=1.84\n",
            "[32 | 537.50] loss=1.88 avg=1.84\n",
            "[33 | 554.07] loss=1.71 avg=1.84\n",
            "[34 | 570.68] loss=1.78 avg=1.84\n",
            "[35 | 587.16] loss=1.82 avg=1.84\n",
            "[36 | 603.88] loss=1.42 avg=1.82\n",
            "[37 | 620.40] loss=1.68 avg=1.82\n",
            "[38 | 636.94] loss=1.71 avg=1.82\n",
            "[39 | 653.60] loss=1.78 avg=1.81\n",
            "[40 | 670.24] loss=2.14 avg=1.82\n",
            "[41 | 686.61] loss=2.16 avg=1.83\n",
            "[42 | 703.18] loss=1.90 avg=1.84\n",
            "[43 | 720.23] loss=2.02 avg=1.84\n",
            "[44 | 737.11] loss=1.49 avg=1.83\n",
            "[45 | 753.26] loss=1.78 avg=1.83\n",
            "[46 | 769.99] loss=1.69 avg=1.83\n",
            "[47 | 786.44] loss=1.89 avg=1.83\n",
            "[48 | 802.86] loss=1.97 avg=1.83\n",
            "[49 | 819.56] loss=1.34 avg=1.82\n",
            "[50 | 835.93] loss=1.21 avg=1.80\n",
            "[51 | 852.79] loss=1.57 avg=1.80\n",
            "[52 | 869.03] loss=1.49 avg=1.79\n",
            "[53 | 885.71] loss=1.68 avg=1.79\n",
            "[54 | 902.34] loss=1.75 avg=1.79\n",
            "[55 | 918.78] loss=1.78 avg=1.79\n",
            "[56 | 935.19] loss=1.64 avg=1.78\n",
            "[57 | 951.35] loss=1.72 avg=1.78\n",
            "[58 | 967.76] loss=2.26 avg=1.79\n",
            "[59 | 984.05] loss=1.83 avg=1.79\n",
            "[60 | 1000.60] loss=1.70 avg=1.79\n",
            "[61 | 1016.89] loss=1.74 avg=1.79\n",
            "[62 | 1034.18] loss=1.91 avg=1.79\n",
            "[63 | 1050.60] loss=1.62 avg=1.79\n",
            "[64 | 1067.24] loss=2.13 avg=1.80\n",
            "[65 | 1083.93] loss=1.49 avg=1.79\n",
            "[66 | 1100.98] loss=2.30 avg=1.80\n",
            "[67 | 1117.65] loss=1.68 avg=1.80\n",
            "[68 | 1134.84] loss=1.56 avg=1.79\n",
            "[69 | 1151.98] loss=1.51 avg=1.79\n",
            "[70 | 1168.48] loss=1.50 avg=1.78\n",
            "[71 | 1185.23] loss=2.21 avg=1.79\n",
            "[72 | 1202.26] loss=1.88 avg=1.79\n",
            "[73 | 1218.87] loss=1.53 avg=1.79\n",
            "[74 | 1235.61] loss=2.14 avg=1.79\n",
            "[75 | 1251.92] loss=1.78 avg=1.79\n",
            "[76 | 1268.61] loss=2.06 avg=1.80\n",
            "[77 | 1285.18] loss=1.95 avg=1.80\n",
            "[78 | 1301.68] loss=1.87 avg=1.80\n",
            "[79 | 1317.77] loss=1.63 avg=1.80\n",
            "[80 | 1334.31] loss=1.69 avg=1.80\n",
            "[81 | 1351.46] loss=1.99 avg=1.80\n",
            "[82 | 1367.74] loss=1.60 avg=1.80\n",
            "[83 | 1384.25] loss=1.46 avg=1.79\n",
            "[84 | 1400.84] loss=1.57 avg=1.79\n",
            "[85 | 1417.34] loss=1.46 avg=1.78\n",
            "[86 | 1433.76] loss=1.58 avg=1.78\n",
            "[87 | 1450.44] loss=1.88 avg=1.78\n",
            "[88 | 1467.12] loss=1.90 avg=1.78\n",
            "[89 | 1483.80] loss=2.48 avg=1.79\n",
            "[90 | 1500.40] loss=1.66 avg=1.79\n",
            "[91 | 1516.76] loss=1.76 avg=1.79\n",
            "[92 | 1533.51] loss=1.58 avg=1.79\n",
            "[93 | 1549.67] loss=1.86 avg=1.79\n",
            "[94 | 1566.09] loss=1.58 avg=1.79\n",
            "[95 | 1582.81] loss=1.63 avg=1.78\n",
            "[96 | 1599.53] loss=1.77 avg=1.78\n",
            "[97 | 1615.98] loss=1.58 avg=1.78\n",
            "[98 | 1632.45] loss=1.77 avg=1.78\n",
            "[99 | 1649.19] loss=1.92 avg=1.78\n",
            "[100 | 1666.52] loss=1.46 avg=1.78\n",
            "[101 | 1682.41] loss=1.69 avg=1.78\n",
            "[102 | 1698.49] loss=1.30 avg=1.77\n",
            "[103 | 1715.00] loss=1.81 avg=1.77\n",
            "[104 | 1731.52] loss=1.88 avg=1.77\n",
            "[105 | 1747.84] loss=2.04 avg=1.77\n",
            "[106 | 1764.10] loss=2.15 avg=1.78\n",
            "[107 | 1780.42] loss=2.06 avg=1.78\n",
            "[108 | 1796.82] loss=1.99 avg=1.79\n",
            "[109 | 1813.20] loss=1.77 avg=1.79\n",
            "[110 | 1829.71] loss=1.81 avg=1.79\n",
            "[111 | 1846.19] loss=1.72 avg=1.79\n",
            "[112 | 1862.48] loss=1.70 avg=1.79\n",
            "[113 | 1878.82] loss=1.70 avg=1.78\n",
            "[114 | 1895.23] loss=1.66 avg=1.78\n",
            "[115 | 1911.89] loss=2.15 avg=1.79\n",
            "[116 | 1927.98] loss=1.89 avg=1.79\n",
            "[117 | 1944.15] loss=2.13 avg=1.79\n",
            "[118 | 1960.85] loss=1.68 avg=1.79\n",
            "[119 | 1977.47] loss=1.65 avg=1.79\n",
            "[120 | 1993.41] loss=1.77 avg=1.79\n",
            "[121 | 2009.46] loss=1.56 avg=1.79\n",
            "[122 | 2026.02] loss=1.84 avg=1.79\n",
            "[123 | 2042.55] loss=1.72 avg=1.79\n",
            "[124 | 2058.86] loss=1.62 avg=1.78\n",
            "[125 | 2075.24] loss=1.61 avg=1.78\n",
            "[126 | 2091.95] loss=1.89 avg=1.78\n",
            "[127 | 2108.44] loss=1.64 avg=1.78\n",
            "[128 | 2125.28] loss=2.10 avg=1.79\n",
            "[129 | 2141.58] loss=1.63 avg=1.78\n",
            "[130 | 2158.11] loss=1.32 avg=1.78\n",
            "[131 | 2174.41] loss=1.67 avg=1.78\n",
            "[132 | 2190.75] loss=1.64 avg=1.77\n",
            "[133 | 2207.38] loss=1.89 avg=1.78\n",
            "[134 | 2223.64] loss=1.64 avg=1.77\n",
            "[135 | 2240.22] loss=2.36 avg=1.78\n",
            "[136 | 2256.57] loss=2.26 avg=1.79\n",
            "[137 | 2273.08] loss=1.48 avg=1.78\n",
            "[138 | 2289.76] loss=2.01 avg=1.79\n",
            "[139 | 2305.89] loss=1.49 avg=1.78\n",
            "[140 | 2322.70] loss=2.00 avg=1.79\n",
            "[141 | 2339.15] loss=1.79 avg=1.79\n",
            "[142 | 2355.31] loss=2.18 avg=1.79\n",
            "[143 | 2371.80] loss=1.50 avg=1.79\n",
            "[144 | 2388.18] loss=1.50 avg=1.78\n",
            "[145 | 2404.55] loss=1.70 avg=1.78\n",
            "[146 | 2420.85] loss=2.11 avg=1.79\n",
            "[147 | 2437.26] loss=1.98 avg=1.79\n",
            "[148 | 2453.24] loss=1.73 avg=1.79\n",
            "[149 | 2469.71] loss=1.73 avg=1.79\n",
            "[150 | 2485.91] loss=1.61 avg=1.79\n",
            "[151 | 2502.32] loss=2.06 avg=1.79\n",
            "[152 | 2518.64] loss=1.92 avg=1.79\n",
            "[153 | 2535.11] loss=1.73 avg=1.79\n",
            "[154 | 2551.77] loss=1.59 avg=1.79\n",
            "[155 | 2568.17] loss=1.66 avg=1.79\n",
            "[156 | 2584.77] loss=2.01 avg=1.79\n",
            "[157 | 2601.55] loss=1.47 avg=1.78\n",
            "[158 | 2618.00] loss=2.12 avg=1.79\n",
            "[159 | 2634.23] loss=1.45 avg=1.78\n",
            "[160 | 2650.79] loss=1.64 avg=1.78\n",
            "[161 | 2666.99] loss=1.92 avg=1.78\n",
            "[162 | 2682.94] loss=1.44 avg=1.78\n",
            "[163 | 2699.59] loss=1.52 avg=1.78\n",
            "[164 | 2716.23] loss=1.74 avg=1.78\n",
            "[165 | 2732.46] loss=1.83 avg=1.78\n",
            "[166 | 2748.74] loss=1.27 avg=1.77\n",
            "[167 | 2764.99] loss=1.39 avg=1.77\n",
            "[168 | 2781.21] loss=1.50 avg=1.76\n",
            "[169 | 2797.54] loss=1.82 avg=1.76\n",
            "[170 | 2814.01] loss=2.12 avg=1.77\n",
            "[171 | 2830.52] loss=1.73 avg=1.77\n",
            "[172 | 2846.93] loss=2.43 avg=1.78\n",
            "[173 | 2862.99] loss=1.87 avg=1.78\n",
            "[174 | 2879.77] loss=1.94 avg=1.78\n",
            "[175 | 2896.42] loss=1.57 avg=1.78\n",
            "[176 | 2913.06] loss=1.89 avg=1.78\n",
            "[177 | 2929.06] loss=1.87 avg=1.78\n",
            "[178 | 2945.47] loss=1.64 avg=1.78\n",
            "[179 | 2962.07] loss=1.93 avg=1.78\n",
            "[180 | 2978.09] loss=1.92 avg=1.78\n",
            "[181 | 2994.79] loss=1.59 avg=1.78\n",
            "[182 | 3011.22] loss=1.85 avg=1.78\n",
            "[183 | 3027.46] loss=1.68 avg=1.78\n",
            "[184 | 3043.80] loss=1.48 avg=1.77\n",
            "[185 | 3059.92] loss=2.10 avg=1.78\n",
            "[186 | 3076.23] loss=1.75 avg=1.78\n",
            "[187 | 3092.26] loss=1.36 avg=1.77\n",
            "[188 | 3108.82] loss=1.56 avg=1.77\n",
            "[189 | 3125.06] loss=1.63 avg=1.77\n",
            "[190 | 3141.68] loss=1.84 avg=1.77\n",
            "[191 | 3158.37] loss=1.97 avg=1.77\n",
            "[192 | 3174.89] loss=1.83 avg=1.77\n",
            "[193 | 3191.44] loss=2.05 avg=1.78\n",
            "[194 | 3208.10] loss=1.73 avg=1.78\n",
            "[195 | 3224.70] loss=2.17 avg=1.78\n",
            "[196 | 3241.31] loss=1.87 avg=1.78\n",
            "[197 | 3257.94] loss=1.97 avg=1.78\n",
            "[198 | 3274.22] loss=2.11 avg=1.79\n",
            "[199 | 3290.79] loss=1.94 avg=1.79\n",
            "[200 | 3307.09] loss=1.48 avg=1.79\n",
            "[201 | 3323.49] loss=1.67 avg=1.78\n",
            "[202 | 3339.70] loss=2.13 avg=1.79\n",
            "[203 | 3356.44] loss=1.34 avg=1.78\n",
            "[204 | 3373.04] loss=1.67 avg=1.78\n",
            "[205 | 3389.39] loss=1.76 avg=1.78\n",
            "[206 | 3405.54] loss=1.44 avg=1.78\n",
            "[207 | 3422.07] loss=1.61 avg=1.78\n",
            "[208 | 3438.70] loss=1.49 avg=1.77\n",
            "[209 | 3455.01] loss=1.78 avg=1.77\n",
            "[210 | 3471.27] loss=1.71 avg=1.77\n",
            "[211 | 3487.64] loss=1.96 avg=1.77\n",
            "[212 | 3504.29] loss=1.72 avg=1.77\n",
            "[213 | 3521.21] loss=1.64 avg=1.77\n",
            "[214 | 3537.82] loss=1.77 avg=1.77\n",
            "[215 | 3554.03] loss=1.47 avg=1.77\n",
            "[216 | 3570.34] loss=1.80 avg=1.77\n",
            "[217 | 3586.86] loss=1.82 avg=1.77\n",
            "[218 | 3603.54] loss=1.58 avg=1.77\n",
            "[219 | 3619.69] loss=1.53 avg=1.76\n",
            "[220 | 3636.25] loss=1.51 avg=1.76\n",
            "[221 | 3652.68] loss=1.55 avg=1.76\n",
            "[222 | 3669.02] loss=1.46 avg=1.76\n",
            "[223 | 3685.80] loss=1.61 avg=1.75\n",
            "[224 | 3702.52] loss=1.43 avg=1.75\n",
            "[225 | 3719.25] loss=1.46 avg=1.75\n",
            "[226 | 3735.92] loss=1.60 avg=1.75\n",
            "[227 | 3752.23] loss=2.00 avg=1.75\n",
            "[228 | 3768.89] loss=1.69 avg=1.75\n",
            "[229 | 3785.48] loss=1.40 avg=1.74\n",
            "[230 | 3802.33] loss=1.49 avg=1.74\n",
            "[231 | 3818.75] loss=1.54 avg=1.74\n",
            "[232 | 3836.10] loss=1.61 avg=1.74\n",
            "[233 | 3852.63] loss=1.72 avg=1.74\n",
            "[234 | 3869.09] loss=1.67 avg=1.74\n",
            "[235 | 3885.34] loss=1.87 avg=1.74\n",
            "[236 | 3901.53] loss=1.39 avg=1.73\n",
            "[237 | 3918.06] loss=1.78 avg=1.73\n",
            "[238 | 3934.58] loss=1.89 avg=1.74\n",
            "[239 | 3951.05] loss=1.95 avg=1.74\n",
            "[240 | 3967.48] loss=1.62 avg=1.74\n",
            "[241 | 3984.01] loss=1.98 avg=1.74\n",
            "[242 | 4000.61] loss=1.96 avg=1.74\n",
            "[243 | 4017.63] loss=2.26 avg=1.75\n",
            "[244 | 4034.21] loss=1.87 avg=1.75\n",
            "[245 | 4050.67] loss=1.71 avg=1.75\n",
            "[246 | 4067.27] loss=1.77 avg=1.75\n",
            "[247 | 4083.18] loss=1.53 avg=1.75\n",
            "[248 | 4099.59] loss=2.21 avg=1.75\n",
            "[249 | 4116.61] loss=1.96 avg=1.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "_+3\\s*(.+2)$ /home/myusername/Documents/VirtualDub/\n",
            "(2007-04-10 23:46:00+00:00) mack_: i'd put ubuntu on the desktop (x11) for a bit then just try xubuntu.\n",
            "\n",
            "\n",
            "(2007-05-10 06:23:00+00:00) zz: oh hi I see you got a problem I have 2 pc\n",
            "(2007-05-10 06:23:00+00:00) zz: I have my 1.6 to connect to a linux-based router but when I do it is still using the 1.6 and when I do it the pc just shuts down\n",
            "(2007-05-10 06:24:00+00:00) zz: my network on mac doesn't work\n",
            "\n",
            "\n",
            "(2006-03-05 21:33:00+00:00) dre: if i download the latest version of ubuntu, it will boot the gimcrack on ubuntu just fine , but i have tried the older versions of ubuntu and it would not boot the gimcrack ... i have this problem but my cpu seems to be ok , but i cant find anything on the issue , its ubuntu and its the same problem i have been having with previous versions\n",
            "(2006-03-05 21:34:00+00:00) dre: you can try to install gimcrack using the command: 'gimcrack-installer --help'\n",
            "(2006-03-05 21:35:00+00:00) dre: not sure what '-v nb' means...\n",
            "(2006-03-05 21:37:00+00:00) mbluemand:  http://ubuntuforums.org/archive/topic.php/t-15.html\n",
            "\n",
            "\n",
            "(2005-03-10 13:37:00+00:00) ikonia1101: hi i found someone... i installed Ubuntu on my laptop and when i go the desktop i get an error saying this is not a universal release for Ubuntu\n",
            "(2005-03-10 13:38:00+00:00) ikonia1101: hi i found someone... i installed Ubuntu on my laptop and when i go the desktop i get an error saying this is not a universal release for Ubuntu\n",
            "(2005-03-10 13:39:00+00:00) ikonia1101: what does ub u get?\n",
            "(2005-03-10 13:40:00+00:00) ikonia1101: i installed ub from a DVD and now it doesn't boot ubuntu\n",
            "\n",
            "\n",
            "(2008-07-06 16:54:00+00:00) c0zt_: hi all : i cant find the correct way to install this :P\n",
            "\n",
            "\n",
            "(2005-11-10 01:29:00+00:00) gsogren: I have an ATI and NVIDIA graphics adapter, but its working fine\n",
            "(2005-11-10 01:30:00+00:00) the_lazy: your right, it'll do it for you. It will take about 30 seconds\n",
            "(2005-11-10 02:23:00+00:00) gg1: my ATI is not working correctly\n",
            "(2005-11-10 02:24:00+00:00) the_lazy: don't like that, not when using an ATI, at all\n",
            "(2005-11-10 02:24:00+00:00) the_lazy: it worked fine\n",
            "(2005-11-10 02:24:00+00:00) the_lazy: i tried installing the drivers again and it worked fine\n",
            "(2005-11-10 02:25:00+00:00) the_lazy: it was just in a hurry but it's a bit old\n",
            "(2005-11-10 02:25:00+00:00) the_lazy: my ATI may be old (2.5 ghz)\n",
            "(2005-11-10 02:26:00+00:00) the_lazy: i've had this problem too, but can't get it to work\n",
            "(2005-11-10 02:26:00+00:00) the_lazy: yes, i have had it done before\n",
            "(2005-11-10 02:26:00+00:00) the_lazy: and i still can't get that\n",
            "(2005-11-10 02:28:00+00:00) the_lazy: if you want to know what to use for the video hdd, there are many ways\n",
            "\n",
            "\n",
            "(2008-04-23 08:52:00+00:00\n",
            "\n",
            "[250 | 4322.11] loss=2.11 avg=1.76\n",
            "[251 | 4337.68] loss=1.53 avg=1.76\n",
            "[252 | 4352.93] loss=1.50 avg=1.75\n",
            "[253 | 4368.23] loss=1.83 avg=1.75\n",
            "[254 | 4383.69] loss=1.69 avg=1.75\n",
            "[255 | 4399.01] loss=2.10 avg=1.76\n",
            "[256 | 4414.11] loss=1.97 avg=1.76\n",
            "[257 | 4429.44] loss=1.56 avg=1.76\n",
            "[258 | 4444.62] loss=1.52 avg=1.75\n",
            "[259 | 4460.44] loss=1.99 avg=1.76\n",
            "[260 | 4476.09] loss=1.99 avg=1.76\n",
            "[261 | 4491.28] loss=1.48 avg=1.76\n",
            "[262 | 4507.15] loss=1.87 avg=1.76\n",
            "[263 | 4523.08] loss=1.35 avg=1.75\n",
            "[264 | 4538.58] loss=2.28 avg=1.76\n",
            "[265 | 4554.11] loss=1.67 avg=1.76\n",
            "[266 | 4569.56] loss=1.88 avg=1.76\n",
            "[267 | 4584.92] loss=1.45 avg=1.76\n",
            "[268 | 4600.24] loss=1.73 avg=1.76\n",
            "[269 | 4615.08] loss=1.59 avg=1.75\n",
            "[270 | 4630.50] loss=1.56 avg=1.75\n",
            "[271 | 4645.94] loss=1.46 avg=1.75\n",
            "[272 | 4661.17] loss=1.73 avg=1.75\n",
            "[273 | 4676.69] loss=1.49 avg=1.75\n",
            "[274 | 4692.00] loss=1.90 avg=1.75\n",
            "[275 | 4707.22] loss=1.89 avg=1.75\n",
            "[276 | 4722.83] loss=1.69 avg=1.75\n",
            "[277 | 4738.20] loss=1.67 avg=1.75\n",
            "[278 | 4753.55] loss=1.76 avg=1.75\n",
            "[279 | 4769.42] loss=1.81 avg=1.75\n",
            "[280 | 4784.84] loss=1.34 avg=1.74\n",
            "[281 | 4800.16] loss=1.96 avg=1.75\n",
            "[282 | 4815.62] loss=2.00 avg=1.75\n",
            "[283 | 4830.96] loss=1.29 avg=1.74\n",
            "[284 | 4845.81] loss=1.56 avg=1.74\n",
            "[285 | 4861.34] loss=2.12 avg=1.75\n",
            "[286 | 4876.78] loss=1.62 avg=1.74\n",
            "[287 | 4892.52] loss=1.95 avg=1.75\n",
            "[288 | 4908.04] loss=1.38 avg=1.74\n",
            "[289 | 4923.43] loss=1.91 avg=1.74\n",
            "[290 | 4938.80] loss=2.25 avg=1.75\n",
            "[291 | 4954.17] loss=1.51 avg=1.75\n",
            "[292 | 4969.66] loss=1.82 avg=1.75\n",
            "[293 | 4984.83] loss=1.53 avg=1.75\n",
            "[294 | 5000.07] loss=1.94 avg=1.75\n",
            "[295 | 5015.52] loss=1.59 avg=1.75\n",
            "[296 | 5030.88] loss=1.82 avg=1.75\n",
            "[297 | 5046.56] loss=1.93 avg=1.75\n",
            "[298 | 5062.00] loss=2.00 avg=1.75\n",
            "[299 | 5078.12] loss=1.93 avg=1.75\n",
            "[300 | 5093.99] loss=1.54 avg=1.75\n",
            "[301 | 5109.41] loss=1.60 avg=1.75\n",
            "[302 | 5125.21] loss=1.61 avg=1.75\n",
            "[303 | 5141.12] loss=1.56 avg=1.75\n",
            "[304 | 5157.05] loss=1.52 avg=1.74\n",
            "[305 | 5172.73] loss=2.24 avg=1.75\n",
            "[306 | 5188.32] loss=2.17 avg=1.75\n",
            "[307 | 5203.87] loss=1.88 avg=1.75\n",
            "[308 | 5219.67] loss=1.97 avg=1.76\n",
            "[309 | 5235.16] loss=1.92 avg=1.76\n",
            "[310 | 5251.15] loss=1.53 avg=1.76\n",
            "[311 | 5266.74] loss=1.91 avg=1.76\n",
            "[312 | 5282.21] loss=1.35 avg=1.75\n",
            "[313 | 5297.92] loss=1.78 avg=1.75\n",
            "[314 | 5313.08] loss=1.53 avg=1.75\n",
            "[315 | 5328.46] loss=2.06 avg=1.75\n",
            "[316 | 5344.19] loss=1.56 avg=1.75\n",
            "[317 | 5359.88] loss=1.76 avg=1.75\n",
            "[318 | 5375.39] loss=2.00 avg=1.76\n",
            "[319 | 5392.14] loss=1.73 avg=1.76\n",
            "[320 | 5407.50] loss=1.59 avg=1.75\n",
            "[321 | 5423.13] loss=1.38 avg=1.75\n",
            "[322 | 5439.15] loss=2.37 avg=1.76\n",
            "[323 | 5455.01] loss=1.69 avg=1.76\n",
            "[324 | 5470.74] loss=1.56 avg=1.75\n",
            "[325 | 5486.28] loss=1.87 avg=1.75\n",
            "[326 | 5501.91] loss=1.93 avg=1.76\n",
            "[327 | 5517.79] loss=1.82 avg=1.76\n",
            "[328 | 5533.41] loss=1.79 avg=1.76\n",
            "[329 | 5548.96] loss=1.99 avg=1.76\n",
            "[330 | 5564.57] loss=1.79 avg=1.76\n",
            "[331 | 5580.08] loss=1.32 avg=1.76\n",
            "[332 | 5595.58] loss=1.56 avg=1.75\n",
            "[333 | 5611.14] loss=1.55 avg=1.75\n",
            "[334 | 5626.50] loss=1.84 avg=1.75\n",
            "[335 | 5641.81] loss=1.70 avg=1.75\n",
            "[336 | 5657.24] loss=1.83 avg=1.75\n",
            "[337 | 5672.69] loss=2.14 avg=1.76\n",
            "[338 | 5688.46] loss=1.56 avg=1.75\n",
            "[339 | 5704.33] loss=1.87 avg=1.76\n",
            "[340 | 5719.47] loss=1.40 avg=1.75\n",
            "[341 | 5734.95] loss=2.29 avg=1.76\n",
            "[342 | 5750.46] loss=2.02 avg=1.76\n",
            "[343 | 5766.13] loss=1.58 avg=1.76\n",
            "[344 | 5782.08] loss=1.85 avg=1.76\n",
            "[345 | 5798.00] loss=1.61 avg=1.76\n",
            "[346 | 5813.42] loss=1.81 avg=1.76\n",
            "[347 | 5828.68] loss=1.68 avg=1.76\n",
            "[348 | 5844.52] loss=1.88 avg=1.76\n",
            "[349 | 5860.06] loss=1.81 avg=1.76\n",
            "[350 | 5875.32] loss=1.59 avg=1.76\n",
            "[351 | 5890.83] loss=1.94 avg=1.76\n",
            "[352 | 5906.37] loss=1.68 avg=1.76\n",
            "[353 | 5921.58] loss=2.25 avg=1.76\n",
            "[354 | 5937.07] loss=1.51 avg=1.76\n",
            "[355 | 5952.05] loss=2.04 avg=1.76\n",
            "[356 | 5967.41] loss=1.39 avg=1.76\n",
            "[357 | 5982.68] loss=1.76 avg=1.76\n",
            "[358 | 5997.81] loss=1.38 avg=1.76\n",
            "[359 | 6013.48] loss=1.95 avg=1.76\n",
            "[360 | 6029.03] loss=2.12 avg=1.76\n",
            "[361 | 6044.58] loss=1.46 avg=1.76\n",
            "[362 | 6060.03] loss=1.68 avg=1.76\n",
            "[363 | 6075.44] loss=1.55 avg=1.76\n",
            "[364 | 6090.30] loss=1.56 avg=1.75\n",
            "[365 | 6105.78] loss=1.44 avg=1.75\n",
            "[366 | 6121.40] loss=1.54 avg=1.75\n",
            "[367 | 6136.88] loss=1.68 avg=1.75\n",
            "[368 | 6151.92] loss=2.32 avg=1.75\n",
            "[369 | 6167.22] loss=1.52 avg=1.75\n",
            "[370 | 6182.71] loss=1.63 avg=1.75\n",
            "[371 | 6198.00] loss=1.48 avg=1.75\n",
            "[372 | 6213.59] loss=2.23 avg=1.75\n",
            "[373 | 6229.04] loss=1.81 avg=1.75\n",
            "[374 | 6244.38] loss=1.83 avg=1.75\n",
            "[375 | 6259.54] loss=2.10 avg=1.76\n",
            "[376 | 6275.03] loss=1.87 avg=1.76\n",
            "[377 | 6290.20] loss=1.69 avg=1.76\n",
            "[378 | 6305.66] loss=1.77 avg=1.76\n",
            "[379 | 6321.83] loss=1.82 avg=1.76\n",
            "[380 | 6337.54] loss=1.80 avg=1.76\n",
            "[381 | 6353.23] loss=1.72 avg=1.76\n",
            "[382 | 6368.62] loss=2.00 avg=1.76\n",
            "[383 | 6384.25] loss=1.92 avg=1.76\n",
            "[384 | 6399.69] loss=2.23 avg=1.77\n",
            "[385 | 6414.99] loss=1.58 avg=1.77\n",
            "[386 | 6430.15] loss=1.86 avg=1.77\n",
            "[387 | 6445.41] loss=1.44 avg=1.76\n",
            "[388 | 6460.81] loss=1.81 avg=1.76\n",
            "[389 | 6476.10] loss=1.58 avg=1.76\n",
            "[390 | 6491.39] loss=1.48 avg=1.76\n",
            "[391 | 6506.82] loss=1.58 avg=1.76\n",
            "[392 | 6522.04] loss=2.15 avg=1.76\n",
            "[393 | 6537.56] loss=1.62 avg=1.76\n",
            "[394 | 6552.66] loss=1.44 avg=1.76\n",
            "[395 | 6568.27] loss=2.22 avg=1.76\n",
            "[396 | 6583.97] loss=1.52 avg=1.76\n",
            "[397 | 6599.43] loss=1.69 avg=1.76\n",
            "[398 | 6614.99] loss=1.98 avg=1.76\n",
            "[399 | 6630.64] loss=1.62 avg=1.76\n",
            "[400 | 6646.23] loss=1.67 avg=1.76\n",
            "[401 | 6661.67] loss=2.14 avg=1.76\n",
            "[402 | 6677.06] loss=1.40 avg=1.76\n",
            "[403 | 6692.26] loss=1.85 avg=1.76\n",
            "[404 | 6707.79] loss=1.78 avg=1.76\n",
            "[405 | 6723.19] loss=1.88 avg=1.76\n",
            "[406 | 6738.65] loss=1.87 avg=1.76\n",
            "[407 | 6754.03] loss=1.87 avg=1.76\n",
            "[408 | 6769.22] loss=1.88 avg=1.76\n",
            "[409 | 6784.48] loss=1.52 avg=1.76\n",
            "[410 | 6799.90] loss=1.56 avg=1.76\n",
            "[411 | 6815.21] loss=2.42 avg=1.77\n",
            "[412 | 6830.73] loss=1.93 avg=1.77\n",
            "[413 | 6845.89] loss=1.86 avg=1.77\n",
            "[414 | 6861.09] loss=1.91 avg=1.77\n",
            "[415 | 6876.03] loss=1.87 avg=1.77\n",
            "[416 | 6891.26] loss=1.89 avg=1.77\n",
            "[417 | 6906.46] loss=1.67 avg=1.77\n",
            "[418 | 6921.89] loss=1.95 avg=1.77\n",
            "[419 | 6936.61] loss=2.12 avg=1.78\n",
            "[420 | 6952.55] loss=1.84 avg=1.78\n",
            "[421 | 6968.29] loss=1.50 avg=1.77\n",
            "[422 | 6983.09] loss=1.69 avg=1.77\n",
            "[423 | 6998.41] loss=1.81 avg=1.77\n",
            "[424 | 7013.84] loss=1.87 avg=1.77\n",
            "[425 | 7029.42] loss=1.39 avg=1.77\n",
            "[426 | 7044.92] loss=2.04 avg=1.77\n",
            "[427 | 7059.99] loss=1.26 avg=1.77\n",
            "[428 | 7074.94] loss=1.79 avg=1.77\n",
            "[429 | 7089.80] loss=1.53 avg=1.77\n",
            "[430 | 7104.88] loss=2.07 avg=1.77\n",
            "[431 | 7120.22] loss=2.01 avg=1.77\n",
            "[432 | 7135.52] loss=1.86 avg=1.77\n",
            "[433 | 7150.89] loss=1.62 avg=1.77\n",
            "[434 | 7166.18] loss=1.73 avg=1.77\n",
            "[435 | 7181.37] loss=2.14 avg=1.77\n",
            "[436 | 7196.91] loss=2.13 avg=1.78\n",
            "[437 | 7212.11] loss=1.64 avg=1.78\n",
            "[438 | 7227.17] loss=1.90 avg=1.78\n",
            "[439 | 7242.33] loss=1.80 avg=1.78\n",
            "[440 | 7258.52] loss=1.82 avg=1.78\n",
            "[441 | 7274.01] loss=1.48 avg=1.78\n",
            "[442 | 7289.23] loss=1.68 avg=1.77\n",
            "[443 | 7304.21] loss=1.67 avg=1.77\n",
            "[444 | 7319.52] loss=1.43 avg=1.77\n",
            "[445 | 7334.86] loss=1.92 avg=1.77\n",
            "[446 | 7350.06] loss=1.96 avg=1.77\n",
            "[447 | 7365.38] loss=2.02 avg=1.78\n",
            "[448 | 7380.73] loss=1.74 avg=1.78\n",
            "[449 | 7395.87] loss=1.28 avg=1.77\n",
            "[450 | 7411.34] loss=1.47 avg=1.77\n",
            "[451 | 7427.01] loss=1.48 avg=1.76\n",
            "[452 | 7442.52] loss=2.03 avg=1.77\n",
            "[453 | 7457.78] loss=1.75 avg=1.77\n",
            "[454 | 7472.84] loss=1.65 avg=1.77\n",
            "[455 | 7488.25] loss=1.55 avg=1.76\n",
            "[456 | 7503.51] loss=1.73 avg=1.76\n",
            "[457 | 7518.88] loss=1.54 avg=1.76\n",
            "[458 | 7533.74] loss=1.83 avg=1.76\n",
            "[459 | 7548.91] loss=1.92 avg=1.76\n",
            "[460 | 7565.11] loss=1.44 avg=1.76\n",
            "[461 | 7581.30] loss=1.81 avg=1.76\n",
            "[462 | 7596.37] loss=1.50 avg=1.76\n",
            "[463 | 7611.82] loss=1.91 avg=1.76\n",
            "[464 | 7626.94] loss=1.70 avg=1.76\n",
            "[465 | 7642.30] loss=1.41 avg=1.76\n",
            "[466 | 7657.47] loss=1.60 avg=1.75\n",
            "[467 | 7673.11] loss=1.35 avg=1.75\n",
            "[468 | 7688.46] loss=1.60 avg=1.75\n",
            "[469 | 7703.56] loss=2.34 avg=1.75\n",
            "[470 | 7719.09] loss=2.04 avg=1.76\n",
            "[471 | 7734.41] loss=1.60 avg=1.76\n",
            "[472 | 7749.65] loss=2.42 avg=1.76\n",
            "[473 | 7765.36] loss=1.60 avg=1.76\n",
            "[474 | 7780.65] loss=1.94 avg=1.76\n",
            "[475 | 7796.22] loss=1.93 avg=1.76\n",
            "[476 | 7811.78] loss=1.79 avg=1.76\n",
            "[477 | 7826.92] loss=2.09 avg=1.77\n",
            "[478 | 7842.24] loss=2.14 avg=1.77\n",
            "[479 | 7857.57] loss=1.92 avg=1.77\n",
            "[480 | 7873.11] loss=1.66 avg=1.77\n",
            "[481 | 7889.20] loss=1.61 avg=1.77\n",
            "[482 | 7904.52] loss=1.46 avg=1.77\n",
            "[483 | 7920.00] loss=1.44 avg=1.76\n",
            "[484 | 7934.97] loss=1.85 avg=1.76\n",
            "[485 | 7950.05] loss=2.05 avg=1.77\n",
            "[486 | 7965.17] loss=1.64 avg=1.77\n",
            "[487 | 7980.59] loss=1.74 avg=1.77\n",
            "[488 | 7996.00] loss=1.48 avg=1.76\n",
            "[489 | 8011.21] loss=1.63 avg=1.76\n",
            "[490 | 8026.74] loss=1.68 avg=1.76\n",
            "[491 | 8042.11] loss=1.95 avg=1.76\n",
            "[492 | 8057.53] loss=2.28 avg=1.77\n",
            "[493 | 8072.72] loss=1.69 avg=1.77\n",
            "[494 | 8087.91] loss=2.09 avg=1.77\n",
            "[495 | 8103.19] loss=1.71 avg=1.77\n",
            "[496 | 8118.60] loss=2.20 avg=1.77\n",
            "[497 | 8133.80] loss=1.74 avg=1.77\n",
            "[498 | 8149.17] loss=1.45 avg=1.77\n",
            "[499 | 8164.60] loss=1.76 avg=1.77\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "00 or\n",
            "(2010-12-08 14:43:00+00:00) D_C: but how much ?\n",
            "(2010-12-08 14:44:00+00:00) mzrz: you can't really see much of a difference\n",
            "(2010-12-08 14:53:00+00:00) mzrz: do we have any reason to be concerned about the file contents?\n",
            "(2010-12-08 14:56:00+00:00) D_C: if you're doing backups (and the like) it shouldn't be really a big deal\n",
            "(2010-12-08 14:56:00+00:00) D_C: well, that could be your question.\n",
            "(2010-12-08 14:57:00+00:00) mzrz: or you can use vim --force to make it much easier than with gd\n",
            "(2010-12-08 14:57:00+00:00) D_C: I just don't use my laptop regularly (I know, I mean, I'm a newbie) and the install was fine.\n",
            "(2010-12-08 15:00:00+00:00) D_C: what is the command?\n",
            "(2010-12-08 15:00:00+00:00) D_C: /home/ubuntu/media/vfwm0.fsmm\n",
            "(2010-12-08 15:01:00+00:00) D_C: well. so, when someone asks 'how do I uninstall vfwm', do I still have vfwm0.fsmm when they run 's/all/home/ubuntu/media/vfwm0.fsmm' ?\n",
            "(2010-12-08 15:05:00+00:00) D_C: you may\n",
            "(2010-12-08 15:06:00+00:00) D_C: maybe, but the fact that it's just '/bin/vfwm0.fsmm does seem to give me more errors when trying to uninstall that, or even just 'notify me of an error'.\n",
            "(2010-12-08 15:07:00+00:00) D_C: no, 'S' not 'S' and not 'S' and 'S'\n",
            "(2010-12-08 15:07:00+00:00) D_C: not my case\n",
            "(2010-12-08 15:07:00+00:00) D_C: I want to install a file, not just a new one, that is, a file created with a vim plugin, for a live window creation\n",
            "(2010-12-08 15:07:00+00:00) mzrz: what's a file created with a vim plugin not?\n",
            "(2010-12-08 15:07:00+00:00) D_C: 'create_window_create_windows' : http://vul.org/2009/02/03/the-vim-plugin-for-gcc-g++/\n",
            "\n",
            "\n",
            "(2006-10-31 07:53:00+00:00) gaius: how do you read a file from a cd ?\n",
            "(2006-10-31 07:55:00+00:00) gaius: I have a disk with files for linux, ubuntu, windows, openoffice, etc\n",
            "(2006-10-31 07:58:00+00:00) gaius: I want to find out how do\n",
            "(2006-10-31 07:58:00+00:00) gaius: where I can find out some info about linux files\n",
            "(2006-10-31 07:58:00+00:00) briannex: is there a file you can find with the find -ls -l files\n",
            "(2006-10-31 08:00:00+00:00) gaius: but I don't know if it would be a 'linux one', I need to know how to get files from windows onto /home /ubuntu\n",
            "(2006-10-31 08:01:00+00:00) gaius: I have windows and my linux drive, can i just go into it, start the win, the files is just the windows drive\n",
            "(2006-10-31 08:01:00+00:00) gaius: where I know if that thing is something to start thinking about\n",
            "(2006-10-31 08:02:00+00:00) gaius: but it's not that... it's like what I need to see something in order to figure out what files are on there\n",
            "(2006-10-31\n",
            "\n",
            "[500 | 8366.29] loss=1.48 avg=1.77\n",
            "[501 | 8381.80] loss=1.54 avg=1.77\n",
            "[502 | 8397.60] loss=1.59 avg=1.76\n",
            "[503 | 8413.67] loss=1.54 avg=1.76\n",
            "[504 | 8429.46] loss=2.13 avg=1.77\n",
            "[505 | 8445.44] loss=1.32 avg=1.76\n",
            "[506 | 8461.06] loss=2.06 avg=1.76\n",
            "[507 | 8476.80] loss=2.42 avg=1.77\n",
            "[508 | 8492.84] loss=1.85 avg=1.77\n",
            "[509 | 8509.37] loss=2.30 avg=1.78\n",
            "[510 | 8524.92] loss=2.22 avg=1.78\n",
            "[511 | 8540.80] loss=1.71 avg=1.78\n",
            "[512 | 8556.48] loss=1.76 avg=1.78\n",
            "[513 | 8572.37] loss=1.90 avg=1.78\n",
            "[514 | 8588.29] loss=2.14 avg=1.78\n",
            "[515 | 8604.24] loss=1.90 avg=1.79\n",
            "[516 | 8620.19] loss=1.58 avg=1.78\n",
            "[517 | 8635.88] loss=1.39 avg=1.78\n",
            "[518 | 8651.96] loss=1.64 avg=1.78\n",
            "[519 | 8667.89] loss=1.63 avg=1.78\n",
            "[520 | 8683.70] loss=0.99 avg=1.77\n",
            "[521 | 8699.44] loss=1.57 avg=1.77\n",
            "[522 | 8715.49] loss=2.10 avg=1.77\n",
            "[523 | 8731.28] loss=1.90 avg=1.77\n",
            "[524 | 8746.89] loss=2.10 avg=1.77\n",
            "[525 | 8762.58] loss=1.59 avg=1.77\n",
            "[526 | 8777.81] loss=2.23 avg=1.78\n",
            "[527 | 8793.68] loss=1.59 avg=1.78\n",
            "[528 | 8809.55] loss=1.83 avg=1.78\n",
            "[529 | 8825.39] loss=1.85 avg=1.78\n",
            "[530 | 8841.10] loss=2.37 avg=1.78\n",
            "[531 | 8856.71] loss=1.84 avg=1.78\n",
            "[532 | 8872.05] loss=2.10 avg=1.79\n",
            "[533 | 8887.64] loss=1.89 avg=1.79\n",
            "[534 | 8903.04] loss=1.99 avg=1.79\n",
            "[535 | 8918.79] loss=2.06 avg=1.79\n",
            "[536 | 8934.44] loss=2.05 avg=1.80\n",
            "[537 | 8950.47] loss=1.64 avg=1.79\n",
            "[538 | 8965.88] loss=1.66 avg=1.79\n",
            "[539 | 8981.67] loss=1.67 avg=1.79\n",
            "[540 | 8997.43] loss=1.30 avg=1.79\n",
            "[541 | 9013.12] loss=1.86 avg=1.79\n",
            "[542 | 9029.02] loss=1.76 avg=1.79\n",
            "[543 | 9044.79] loss=1.79 avg=1.79\n",
            "[544 | 9060.39] loss=1.85 avg=1.79\n",
            "[545 | 9076.12] loss=1.23 avg=1.78\n",
            "[546 | 9092.09] loss=2.10 avg=1.78\n",
            "[547 | 9107.58] loss=1.71 avg=1.78\n",
            "[548 | 9123.71] loss=1.52 avg=1.78\n",
            "[549 | 9139.45] loss=2.04 avg=1.78\n",
            "[550 | 9155.30] loss=1.65 avg=1.78\n",
            "[551 | 9170.93] loss=1.89 avg=1.78\n",
            "[552 | 9186.98] loss=1.76 avg=1.78\n",
            "[553 | 9202.39] loss=1.26 avg=1.78\n",
            "[554 | 9218.03] loss=1.92 avg=1.78\n",
            "[555 | 9233.73] loss=1.97 avg=1.78\n",
            "[556 | 9249.36] loss=1.63 avg=1.78\n",
            "[557 | 9265.11] loss=1.23 avg=1.77\n",
            "[558 | 9281.13] loss=1.59 avg=1.77\n",
            "[559 | 9296.90] loss=1.45 avg=1.77\n",
            "[560 | 9312.72] loss=1.89 avg=1.77\n",
            "[561 | 9328.40] loss=1.27 avg=1.77\n",
            "[562 | 9344.26] loss=1.63 avg=1.76\n",
            "[563 | 9360.07] loss=1.98 avg=1.77\n",
            "[564 | 9376.14] loss=1.64 avg=1.77\n",
            "[565 | 9391.97] loss=1.88 avg=1.77\n",
            "[566 | 9408.12] loss=1.58 avg=1.76\n",
            "[567 | 9424.15] loss=1.72 avg=1.76\n",
            "[568 | 9441.29] loss=1.67 avg=1.76\n",
            "[569 | 9457.37] loss=1.62 avg=1.76\n",
            "[570 | 9473.18] loss=1.53 avg=1.76\n",
            "[571 | 9488.80] loss=1.79 avg=1.76\n",
            "[572 | 9504.90] loss=1.65 avg=1.76\n",
            "[573 | 9520.48] loss=1.59 avg=1.76\n",
            "[574 | 9536.39] loss=1.86 avg=1.76\n",
            "[575 | 9551.98] loss=1.92 avg=1.76\n",
            "[576 | 9567.82] loss=1.66 avg=1.76\n",
            "[577 | 9583.42] loss=1.37 avg=1.75\n",
            "[578 | 9599.96] loss=1.97 avg=1.76\n",
            "[579 | 9615.42] loss=1.81 avg=1.76\n",
            "[580 | 9630.90] loss=1.92 avg=1.76\n",
            "[581 | 9646.41] loss=1.75 avg=1.76\n",
            "[582 | 9662.27] loss=1.90 avg=1.76\n",
            "[583 | 9678.33] loss=1.69 avg=1.76\n",
            "[584 | 9694.20] loss=1.95 avg=1.76\n",
            "[585 | 9710.10] loss=1.44 avg=1.76\n",
            "[586 | 9726.21] loss=1.90 avg=1.76\n",
            "[587 | 9742.38] loss=1.89 avg=1.76\n",
            "[588 | 9758.60] loss=1.47 avg=1.76\n",
            "[589 | 9774.61] loss=2.15 avg=1.76\n",
            "[590 | 9790.13] loss=1.79 avg=1.76\n",
            "[591 | 9806.06] loss=1.59 avg=1.76\n",
            "[592 | 9821.78] loss=1.89 avg=1.76\n",
            "[593 | 9837.34] loss=1.98 avg=1.76\n",
            "[594 | 9852.91] loss=1.92 avg=1.77\n",
            "[595 | 9868.46] loss=1.77 avg=1.77\n",
            "[596 | 9884.48] loss=1.46 avg=1.76\n",
            "[597 | 9900.21] loss=1.53 avg=1.76\n",
            "[598 | 9916.26] loss=1.55 avg=1.76\n",
            "[599 | 9931.88] loss=1.65 avg=1.76\n",
            "[600 | 9947.53] loss=2.05 avg=1.76\n",
            "[601 | 9963.84] loss=1.67 avg=1.76\n",
            "[602 | 9979.44] loss=1.84 avg=1.76\n",
            "[603 | 9995.54] loss=1.79 avg=1.76\n",
            "[604 | 10011.50] loss=1.62 avg=1.76\n",
            "[605 | 10027.55] loss=1.65 avg=1.76\n",
            "[606 | 10043.35] loss=1.55 avg=1.76\n",
            "[607 | 10060.24] loss=1.78 avg=1.76\n",
            "[608 | 10075.77] loss=1.66 avg=1.75\n",
            "[609 | 10091.62] loss=1.44 avg=1.75\n",
            "[610 | 10107.67] loss=2.04 avg=1.75\n",
            "[611 | 10123.75] loss=1.48 avg=1.75\n",
            "[612 | 10139.65] loss=1.71 avg=1.75\n",
            "[613 | 10155.45] loss=2.06 avg=1.75\n",
            "[614 | 10171.15] loss=2.23 avg=1.76\n",
            "[615 | 10187.13] loss=2.05 avg=1.76\n",
            "[616 | 10203.11] loss=1.43 avg=1.76\n",
            "[617 | 10219.04] loss=1.90 avg=1.76\n",
            "[618 | 10235.06] loss=1.39 avg=1.76\n",
            "[619 | 10250.77] loss=1.54 avg=1.75\n",
            "[620 | 10267.15] loss=2.04 avg=1.76\n",
            "[621 | 10283.30] loss=1.69 avg=1.76\n",
            "[622 | 10299.16] loss=1.96 avg=1.76\n",
            "[623 | 10315.22] loss=2.16 avg=1.76\n",
            "[624 | 10331.49] loss=1.48 avg=1.76\n",
            "[625 | 10347.32] loss=2.11 avg=1.76\n",
            "[626 | 10363.45] loss=1.55 avg=1.76\n",
            "[627 | 10380.24] loss=1.23 avg=1.76\n",
            "[628 | 10396.26] loss=1.82 avg=1.76\n",
            "[629 | 10412.24] loss=2.02 avg=1.76\n",
            "[630 | 10428.44] loss=1.92 avg=1.76\n",
            "[631 | 10444.57] loss=1.03 avg=1.75\n",
            "[632 | 10460.55] loss=1.55 avg=1.75\n",
            "[633 | 10476.58] loss=1.86 avg=1.75\n",
            "[634 | 10492.94] loss=1.51 avg=1.75\n",
            "[635 | 10509.06] loss=1.58 avg=1.75\n",
            "[636 | 10524.94] loss=1.64 avg=1.75\n",
            "[637 | 10541.43] loss=1.90 avg=1.75\n",
            "[638 | 10557.65] loss=1.80 avg=1.75\n",
            "[639 | 10573.70] loss=1.96 avg=1.75\n",
            "[640 | 10589.24] loss=1.69 avg=1.75\n",
            "[641 | 10605.42] loss=2.08 avg=1.75\n",
            "[642 | 10621.57] loss=2.20 avg=1.76\n",
            "[643 | 10637.90] loss=1.42 avg=1.76\n",
            "[644 | 10654.93] loss=1.94 avg=1.76\n",
            "[645 | 10671.10] loss=1.58 avg=1.76\n",
            "[646 | 10688.51] loss=1.22 avg=1.75\n",
            "[647 | 10704.55] loss=1.57 avg=1.75\n",
            "[648 | 10720.73] loss=1.69 avg=1.75\n",
            "[649 | 10736.69] loss=2.11 avg=1.75\n",
            "[650 | 10752.43] loss=1.87 avg=1.75\n",
            "[651 | 10767.98] loss=1.52 avg=1.75\n",
            "[652 | 10784.16] loss=1.68 avg=1.75\n",
            "[653 | 10800.36] loss=1.81 avg=1.75\n",
            "[654 | 10816.51] loss=1.73 avg=1.75\n",
            "[655 | 10832.46] loss=1.92 avg=1.75\n",
            "[656 | 10848.35] loss=1.80 avg=1.75\n",
            "[657 | 10863.97] loss=1.97 avg=1.75\n",
            "[658 | 10880.04] loss=1.91 avg=1.76\n",
            "[659 | 10895.20] loss=1.90 avg=1.76\n",
            "[660 | 10911.19] loss=1.77 avg=1.76\n",
            "[661 | 10926.82] loss=1.85 avg=1.76\n",
            "[662 | 10943.15] loss=2.02 avg=1.76\n",
            "[663 | 10959.12] loss=1.77 avg=1.76\n",
            "[664 | 10975.23] loss=1.91 avg=1.76\n",
            "[665 | 10991.79] loss=2.14 avg=1.77\n",
            "[666 | 11007.34] loss=1.70 avg=1.77\n",
            "[667 | 11022.79] loss=1.42 avg=1.76\n",
            "[668 | 11038.70] loss=1.85 avg=1.76\n",
            "[669 | 11054.92] loss=1.70 avg=1.76\n",
            "[670 | 11070.96] loss=2.01 avg=1.76\n",
            "[671 | 11086.94] loss=2.02 avg=1.77\n",
            "[672 | 11102.68] loss=1.89 avg=1.77\n",
            "[673 | 11118.22] loss=2.01 avg=1.77\n",
            "[674 | 11133.99] loss=1.79 avg=1.77\n",
            "[675 | 11149.94] loss=2.20 avg=1.78\n",
            "[676 | 11165.36] loss=1.79 avg=1.78\n",
            "[677 | 11181.09] loss=1.72 avg=1.78\n",
            "[678 | 11196.80] loss=1.76 avg=1.77\n",
            "[679 | 11212.82] loss=2.16 avg=1.78\n",
            "[680 | 11228.69] loss=1.77 avg=1.78\n",
            "[681 | 11244.70] loss=1.55 avg=1.78\n",
            "[682 | 11260.51] loss=1.80 avg=1.78\n",
            "[683 | 11276.82] loss=1.55 avg=1.77\n",
            "[684 | 11292.54] loss=2.08 avg=1.78\n",
            "[685 | 11308.78] loss=1.45 avg=1.77\n",
            "[686 | 11324.60] loss=2.24 avg=1.78\n",
            "[687 | 11340.89] loss=1.88 avg=1.78\n",
            "[688 | 11356.66] loss=1.38 avg=1.78\n",
            "[689 | 11372.80] loss=1.46 avg=1.77\n",
            "[690 | 11388.78] loss=1.62 avg=1.77\n",
            "[691 | 11404.88] loss=2.10 avg=1.77\n",
            "[692 | 11420.20] loss=1.64 avg=1.77\n",
            "[693 | 11436.13] loss=2.02 avg=1.78\n",
            "[694 | 11452.30] loss=1.95 avg=1.78\n",
            "[695 | 11468.31] loss=2.04 avg=1.78\n",
            "[696 | 11484.23] loss=1.42 avg=1.78\n",
            "[697 | 11500.16] loss=1.33 avg=1.77\n",
            "[698 | 11516.05] loss=1.50 avg=1.77\n",
            "[699 | 11531.48] loss=1.24 avg=1.76\n",
            "[700 | 11548.00] loss=2.04 avg=1.77\n",
            "[701 | 11563.74] loss=1.76 avg=1.77\n",
            "[702 | 11579.83] loss=1.53 avg=1.76\n",
            "[703 | 11595.93] loss=1.73 avg=1.76\n",
            "[704 | 11612.48] loss=1.88 avg=1.76\n",
            "[705 | 11628.42] loss=1.66 avg=1.76\n",
            "[706 | 11644.10] loss=2.09 avg=1.77\n",
            "[707 | 11659.97] loss=1.76 avg=1.77\n",
            "[708 | 11675.81] loss=1.82 avg=1.77\n",
            "[709 | 11691.78] loss=1.89 avg=1.77\n",
            "[710 | 11707.76] loss=1.56 avg=1.77\n",
            "[711 | 11723.98] loss=2.00 avg=1.77\n",
            "[712 | 11739.76] loss=1.54 avg=1.77\n",
            "[713 | 11755.50] loss=1.69 avg=1.77\n",
            "[714 | 11771.46] loss=1.41 avg=1.76\n",
            "[715 | 11787.23] loss=1.72 avg=1.76\n",
            "[716 | 11802.85] loss=1.58 avg=1.76\n",
            "[717 | 11818.64] loss=1.48 avg=1.76\n",
            "[718 | 11834.24] loss=2.44 avg=1.76\n",
            "[719 | 11850.08] loss=2.10 avg=1.77\n",
            "[720 | 11866.01] loss=1.96 avg=1.77\n",
            "[721 | 11881.88] loss=1.91 avg=1.77\n",
            "[722 | 11897.52] loss=1.58 avg=1.77\n",
            "[723 | 11913.54] loss=1.61 avg=1.77\n",
            "[724 | 11930.35] loss=1.89 avg=1.77\n",
            "[725 | 11946.43] loss=1.79 avg=1.77\n",
            "[726 | 11962.66] loss=1.87 avg=1.77\n",
            "[727 | 11978.26] loss=1.49 avg=1.77\n",
            "[728 | 11994.14] loss=1.85 avg=1.77\n",
            "[729 | 12009.90] loss=1.29 avg=1.76\n",
            "[730 | 12025.84] loss=1.53 avg=1.76\n",
            "[731 | 12042.04] loss=1.49 avg=1.76\n",
            "[732 | 12057.92] loss=1.67 avg=1.76\n",
            "[733 | 12073.64] loss=1.50 avg=1.75\n",
            "[734 | 12089.55] loss=1.40 avg=1.75\n",
            "[735 | 12105.34] loss=2.21 avg=1.76\n",
            "[736 | 12120.98] loss=1.75 avg=1.76\n",
            "[737 | 12137.50] loss=1.87 avg=1.76\n",
            "[738 | 12153.21] loss=1.28 avg=1.75\n",
            "[739 | 12169.15] loss=1.59 avg=1.75\n",
            "[740 | 12184.93] loss=1.47 avg=1.75\n",
            "[741 | 12200.84] loss=1.42 avg=1.74\n",
            "[742 | 12216.32] loss=1.73 avg=1.74\n",
            "[743 | 12232.53] loss=1.89 avg=1.75\n",
            "[744 | 12248.95] loss=1.52 avg=1.74\n",
            "[745 | 12265.21] loss=1.99 avg=1.75\n",
            "[746 | 12280.12] loss=1.55 avg=1.74\n",
            "[747 | 12295.80] loss=1.84 avg=1.74\n",
            "[748 | 12311.60] loss=1.77 avg=1.75\n",
            "[749 | 12327.34] loss=2.00 avg=1.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " way I've changed its behaviour, so it's back to default but it won't remember it's state unless I enable logging\n",
            "(2006-11-02 10:57:00+00:00) meyar: that's the first step from which I can change it's behaviour\n",
            "(2006-11-02 10:57:00+00:00) meyar: and I can still run an application that relies on other processes to start/stop\n",
            "(2006-11-02 10:58:00+00:00) jp: I changed its behaviour, that's the last step\n",
            "(2006-11-02 10:58:00+00:00) meyar: the same... but when you run your desktop app, it will start automatically... and you don't need to log at the start of each task...\n",
            "(2006-11-02 10:58:00+00:00) meyar: what if you do it in a terminal?\n",
            "(2006-11-02 10:59:00+00:00) meyar: it's a command line app\n",
            "(2006-11-02 10:59:00+00:00) meyar: it doesn't start in any terminal\n",
            "(2006-11-02 10:59:00+00:00) jp: It's not starting in your desktop app?\n",
            "(2006-11-02 10:59:00+00:00) meyar: No, it starts in command line\n",
            "(2006-11-02 10:59:00+00:00) jp: It's like 'desktop' to me...\n",
            "(2006-11-02 10:59:00+00:00) meyar: That's why I did it on the commandline\n",
            "(2006-11-02 11:59:00+00:00) meyar: I think you are using an external drive\n",
            "(2006-11-02 12:00:00+00:00) meyar: It's not on an external drive. That's why it's on command line. As far as I know, it starts by itself in your terminal\n",
            "(2006-11-02 12:00:00+00:00) jp: It is a command line app that's running. That means it runs only when you use the sudo command to start it.\n",
            "(2006-11-02 12:00:00+00:00) meyar: It's an 'external drive' :)\n",
            "(2006-11-02 12:01:00+00:00) meyar: Not on an external drive either. On command line, it starts the command line program and starts it and it isn't started in a terminal\n",
            "\n",
            "\n",
            "(2008-03-19 13:34:00+00:00) shikar__: what is the difference between 'sudo apt-get' and 'dpkg-get'?\n",
            "(2008-03-19 13:35:00+00:00) c-k-: dpkg-installed\n",
            "(2008-03-19 13:36:00+00:00) shikar__:  i have 'sudo dpkg-get' then when i attempt to 'dpkg-get' or any of the other 'sudo apt-get' commands, it does not tell me if it's installed or not\n",
            "(2008-03-19 13:36:00+00:00) C-K-: what does 'dpkg-get' have to do with the system?\n",
            "(2008-03-19 13:36:00+00:00) shikar__: it is the most recent command on your system\n",
            "(2008-03-19 13:37:00+00:00) c-k-: no, dpkg-get will just give you the list of packages.   So you're just getting the list of package names\n",
            "(2008-03-05 01:22:00+00:00) shikar__: so i'm not confused about the command i type?\n",
            "(2008-03-05 02:08:00+00:00) c-k-: it's not dpkg-get\n",
            "(2008-03-05 02:08:00+00:00) shikar__: i had it installed from kde already, so i don't think i have to do anything at all...\n",
            "(2008-03-05 02:11:00+00:00) shikar__: i've already installed the packages.  So it doesn't ask me if i'm installing them correctly or not\n",
            "(2008-03-05 02:11:00+00:00) shikar__: is this it\n",
            "(2008-03-05 02:16:00+00:00\n",
            "\n",
            "[750 | 12526.64] loss=1.42 avg=1.74\n",
            "[751 | 12542.39] loss=1.64 avg=1.74\n",
            "[752 | 12558.46] loss=2.13 avg=1.75\n",
            "[753 | 12573.71] loss=2.08 avg=1.75\n",
            "[754 | 12589.51] loss=1.71 avg=1.75\n",
            "[755 | 12605.18] loss=1.62 avg=1.75\n",
            "[756 | 12620.88] loss=1.80 avg=1.75\n",
            "[757 | 12636.26] loss=1.90 avg=1.75\n",
            "[758 | 12651.72] loss=1.41 avg=1.75\n",
            "[759 | 12667.30] loss=1.86 avg=1.75\n",
            "[760 | 12682.77] loss=1.81 avg=1.75\n",
            "[761 | 12698.43] loss=2.06 avg=1.75\n",
            "[762 | 12713.89] loss=1.70 avg=1.75\n",
            "[763 | 12729.86] loss=1.81 avg=1.75\n",
            "[764 | 12745.49] loss=1.80 avg=1.75\n",
            "[765 | 12760.97] loss=1.58 avg=1.75\n",
            "[766 | 12776.57] loss=1.56 avg=1.75\n",
            "[767 | 12792.30] loss=1.86 avg=1.75\n",
            "[768 | 12807.98] loss=1.45 avg=1.75\n",
            "[769 | 12823.59] loss=1.50 avg=1.74\n",
            "[770 | 12839.15] loss=1.47 avg=1.74\n",
            "[771 | 12855.03] loss=1.68 avg=1.74\n",
            "[772 | 12871.06] loss=1.70 avg=1.74\n",
            "[773 | 12886.48] loss=1.64 avg=1.74\n",
            "[774 | 12901.74] loss=1.99 avg=1.74\n",
            "[775 | 12917.28] loss=2.10 avg=1.75\n",
            "[776 | 12932.61] loss=1.73 avg=1.75\n",
            "[777 | 12947.97] loss=2.07 avg=1.75\n",
            "[778 | 12963.48] loss=1.48 avg=1.75\n",
            "[779 | 12979.29] loss=1.26 avg=1.74\n",
            "[780 | 12994.94] loss=1.68 avg=1.74\n",
            "[781 | 13010.45] loss=2.10 avg=1.74\n",
            "[782 | 13025.85] loss=2.08 avg=1.75\n",
            "[783 | 13041.50] loss=1.76 avg=1.75\n",
            "[784 | 13057.10] loss=2.21 avg=1.75\n",
            "[785 | 13072.84] loss=1.71 avg=1.75\n",
            "[786 | 13088.30] loss=1.73 avg=1.75\n",
            "[787 | 13104.04] loss=1.97 avg=1.75\n",
            "[788 | 13119.79] loss=1.23 avg=1.75\n",
            "[789 | 13135.19] loss=1.71 avg=1.75\n",
            "[790 | 13150.53] loss=1.66 avg=1.75\n",
            "[791 | 13166.64] loss=1.91 avg=1.75\n",
            "[792 | 13182.15] loss=1.68 avg=1.75\n",
            "[793 | 13197.78] loss=2.05 avg=1.75\n",
            "[794 | 13213.16] loss=1.62 avg=1.75\n",
            "[795 | 13228.85] loss=1.55 avg=1.75\n",
            "[796 | 13244.31] loss=1.54 avg=1.75\n",
            "[797 | 13259.97] loss=1.97 avg=1.75\n",
            "[798 | 13275.64] loss=1.78 avg=1.75\n",
            "[799 | 13291.13] loss=2.18 avg=1.75\n",
            "[800 | 13306.87] loss=1.43 avg=1.75\n",
            "[801 | 13322.39] loss=1.56 avg=1.75\n",
            "[802 | 13338.20] loss=2.29 avg=1.75\n",
            "[803 | 13353.28] loss=1.74 avg=1.75\n",
            "[804 | 13369.20] loss=1.97 avg=1.76\n",
            "[805 | 13384.92] loss=2.09 avg=1.76\n",
            "[806 | 13400.23] loss=1.59 avg=1.76\n",
            "[807 | 13415.75] loss=1.57 avg=1.76\n",
            "[808 | 13431.12] loss=1.94 avg=1.76\n",
            "[809 | 13446.48] loss=1.51 avg=1.75\n",
            "[810 | 13461.94] loss=1.87 avg=1.76\n",
            "[811 | 13477.92] loss=2.07 avg=1.76\n",
            "[812 | 13493.94] loss=1.78 avg=1.76\n",
            "[813 | 13509.42] loss=1.61 avg=1.76\n",
            "[814 | 13524.93] loss=1.53 avg=1.76\n",
            "[815 | 13540.56] loss=1.65 avg=1.75\n",
            "[816 | 13556.04] loss=1.91 avg=1.76\n",
            "[817 | 13571.76] loss=1.36 avg=1.75\n",
            "[818 | 13587.46] loss=2.00 avg=1.75\n",
            "[819 | 13602.62] loss=1.58 avg=1.75\n",
            "[820 | 13618.38] loss=1.90 avg=1.75\n",
            "[821 | 13634.07] loss=1.63 avg=1.75\n",
            "[822 | 13649.30] loss=1.89 avg=1.75\n",
            "[823 | 13664.42] loss=1.41 avg=1.75\n",
            "[824 | 13680.04] loss=1.35 avg=1.75\n",
            "[825 | 13695.84] loss=1.62 avg=1.75\n",
            "[826 | 13711.34] loss=1.79 avg=1.75\n",
            "[827 | 13726.94] loss=2.09 avg=1.75\n",
            "[828 | 13740.51] loss=1.58 avg=1.75\n",
            "[829 | 13756.48] loss=1.97 avg=1.75\n",
            "[830 | 13772.37] loss=1.48 avg=1.75\n",
            "[831 | 13788.57] loss=1.73 avg=1.75\n",
            "[832 | 13804.69] loss=1.61 avg=1.75\n",
            "[833 | 13820.31] loss=1.97 avg=1.75\n",
            "[834 | 13836.09] loss=1.97 avg=1.75\n",
            "[835 | 13851.68] loss=1.75 avg=1.75\n",
            "[836 | 13867.14] loss=1.70 avg=1.75\n",
            "[837 | 13882.72] loss=1.56 avg=1.75\n",
            "[838 | 13897.99] loss=2.07 avg=1.75\n",
            "[839 | 13913.51] loss=1.60 avg=1.75\n",
            "[840 | 13928.71] loss=1.75 avg=1.75\n",
            "[841 | 13944.21] loss=1.63 avg=1.75\n",
            "[842 | 13959.96] loss=1.59 avg=1.75\n",
            "[843 | 13975.47] loss=1.37 avg=1.74\n",
            "[844 | 13991.54] loss=1.56 avg=1.74\n",
            "[845 | 14006.72] loss=1.95 avg=1.74\n",
            "[846 | 14022.25] loss=1.52 avg=1.74\n",
            "[847 | 14037.81] loss=1.49 avg=1.74\n",
            "[848 | 14053.59] loss=1.54 avg=1.74\n",
            "[849 | 14069.36] loss=1.53 avg=1.73\n",
            "[850 | 14084.88] loss=1.26 avg=1.73\n",
            "[851 | 14100.84] loss=1.69 avg=1.73\n",
            "[852 | 14116.40] loss=1.86 avg=1.73\n",
            "[853 | 14131.57] loss=2.07 avg=1.73\n",
            "[854 | 14147.03] loss=1.76 avg=1.73\n",
            "[855 | 14162.68] loss=1.93 avg=1.74\n",
            "[856 | 14178.18] loss=1.32 avg=1.73\n",
            "[857 | 14193.61] loss=1.32 avg=1.73\n",
            "[858 | 14209.18] loss=1.80 avg=1.73\n",
            "[859 | 14224.54] loss=1.87 avg=1.73\n",
            "[860 | 14240.25] loss=1.57 avg=1.73\n",
            "[861 | 14255.55] loss=1.37 avg=1.73\n",
            "[862 | 14271.11] loss=1.82 avg=1.73\n",
            "[863 | 14287.21] loss=1.86 avg=1.73\n",
            "[864 | 14302.89] loss=1.69 avg=1.73\n",
            "[865 | 14318.46] loss=1.85 avg=1.73\n",
            "[866 | 14334.11] loss=1.49 avg=1.73\n",
            "[867 | 14349.92] loss=1.72 avg=1.73\n",
            "[868 | 14365.73] loss=1.76 avg=1.73\n",
            "[869 | 14381.49] loss=1.71 avg=1.73\n",
            "[870 | 14396.93] loss=1.64 avg=1.73\n",
            "[871 | 14413.20] loss=2.10 avg=1.73\n",
            "[872 | 14428.90] loss=1.43 avg=1.73\n",
            "[873 | 14444.28] loss=1.99 avg=1.73\n",
            "[874 | 14459.91] loss=1.70 avg=1.73\n",
            "[875 | 14475.45] loss=1.66 avg=1.73\n",
            "[876 | 14491.32] loss=1.72 avg=1.73\n",
            "[877 | 14506.88] loss=2.18 avg=1.73\n",
            "[878 | 14522.68] loss=1.62 avg=1.73\n",
            "[879 | 14538.34] loss=1.45 avg=1.73\n",
            "[880 | 14554.01] loss=1.47 avg=1.73\n",
            "[881 | 14569.32] loss=1.72 avg=1.73\n",
            "[882 | 14585.08] loss=1.96 avg=1.73\n",
            "[883 | 14600.88] loss=1.28 avg=1.72\n",
            "[884 | 14616.77] loss=1.90 avg=1.72\n",
            "[885 | 14632.43] loss=2.11 avg=1.73\n",
            "[886 | 14647.82] loss=1.29 avg=1.72\n",
            "[887 | 14663.45] loss=1.90 avg=1.73\n",
            "[888 | 14678.95] loss=1.84 avg=1.73\n",
            "[889 | 14694.62] loss=1.80 avg=1.73\n",
            "[890 | 14710.28] loss=1.57 avg=1.73\n",
            "[891 | 14726.55] loss=1.78 avg=1.73\n",
            "[892 | 14742.02] loss=1.65 avg=1.73\n",
            "[893 | 14757.69] loss=1.47 avg=1.72\n",
            "[894 | 14773.29] loss=1.88 avg=1.73\n",
            "[895 | 14788.76] loss=1.72 avg=1.73\n",
            "[896 | 14804.40] loss=1.37 avg=1.72\n",
            "[897 | 14820.05] loss=1.46 avg=1.72\n",
            "[898 | 14836.03] loss=1.46 avg=1.72\n",
            "[899 | 14851.83] loss=1.48 avg=1.71\n",
            "[900 | 14867.68] loss=1.39 avg=1.71\n",
            "[901 | 14883.52] loss=1.56 avg=1.71\n",
            "[902 | 14899.12] loss=1.63 avg=1.71\n",
            "[903 | 14914.96] loss=1.49 avg=1.71\n",
            "[904 | 14930.36] loss=1.62 avg=1.71\n",
            "[905 | 14945.94] loss=2.54 avg=1.71\n",
            "[906 | 14962.04] loss=1.66 avg=1.71\n",
            "[907 | 14978.02] loss=1.49 avg=1.71\n",
            "[908 | 14993.62] loss=1.67 avg=1.71\n",
            "[909 | 15009.34] loss=1.81 avg=1.71\n",
            "[910 | 15024.69] loss=1.88 avg=1.71\n",
            "[911 | 15041.06] loss=1.25 avg=1.71\n",
            "[912 | 15056.68] loss=1.80 avg=1.71\n",
            "[913 | 15072.35] loss=2.04 avg=1.71\n",
            "[914 | 15087.91] loss=1.61 avg=1.71\n",
            "[915 | 15103.37] loss=1.44 avg=1.71\n",
            "[916 | 15119.06] loss=1.45 avg=1.71\n",
            "[917 | 15134.72] loss=1.75 avg=1.71\n",
            "[918 | 15150.39] loss=1.53 avg=1.71\n",
            "[919 | 15166.03] loss=2.09 avg=1.71\n",
            "[920 | 15181.07] loss=1.98 avg=1.71\n",
            "[921 | 15196.70] loss=1.57 avg=1.71\n",
            "[922 | 15212.73] loss=1.59 avg=1.71\n",
            "[923 | 15228.68] loss=1.75 avg=1.71\n",
            "[924 | 15243.98] loss=1.75 avg=1.71\n",
            "[925 | 15259.65] loss=2.11 avg=1.71\n",
            "[926 | 15275.32] loss=1.62 avg=1.71\n",
            "[927 | 15290.44] loss=1.55 avg=1.71\n",
            "[928 | 15306.16] loss=1.53 avg=1.71\n",
            "[929 | 15321.86] loss=1.89 avg=1.71\n",
            "[930 | 15336.95] loss=1.54 avg=1.71\n",
            "[931 | 15353.29] loss=1.95 avg=1.71\n",
            "[932 | 15369.06] loss=2.20 avg=1.72\n",
            "[933 | 15384.82] loss=1.72 avg=1.72\n",
            "[934 | 15400.06] loss=2.15 avg=1.72\n",
            "[935 | 15415.50] loss=1.77 avg=1.72\n",
            "[936 | 15431.12] loss=2.02 avg=1.72\n",
            "[937 | 15446.77] loss=1.58 avg=1.72\n",
            "[938 | 15462.49] loss=1.94 avg=1.73\n",
            "[939 | 15478.25] loss=1.80 avg=1.73\n",
            "[940 | 15493.83] loss=1.97 avg=1.73\n",
            "[941 | 15509.21] loss=1.70 avg=1.73\n",
            "[942 | 15525.26] loss=1.61 avg=1.73\n",
            "[943 | 15540.91] loss=1.40 avg=1.72\n",
            "[944 | 15556.39] loss=1.59 avg=1.72\n",
            "[945 | 15571.70] loss=1.47 avg=1.72\n",
            "[946 | 15587.23] loss=1.50 avg=1.72\n",
            "[947 | 15602.97] loss=1.74 avg=1.72\n",
            "[948 | 15618.49] loss=1.66 avg=1.72\n",
            "[949 | 15633.84] loss=1.64 avg=1.72\n",
            "[950 | 15649.68] loss=2.23 avg=1.72\n",
            "[951 | 15666.25] loss=1.68 avg=1.72\n",
            "[952 | 15681.75] loss=2.27 avg=1.73\n",
            "[953 | 15697.37] loss=1.64 avg=1.73\n",
            "[954 | 15713.07] loss=1.23 avg=1.72\n",
            "[955 | 15728.74] loss=1.96 avg=1.72\n",
            "[956 | 15744.50] loss=2.04 avg=1.73\n",
            "[957 | 15760.34] loss=2.05 avg=1.73\n",
            "[958 | 15776.10] loss=2.10 avg=1.73\n",
            "[959 | 15791.69] loss=2.11 avg=1.74\n",
            "[960 | 15807.39] loss=1.92 avg=1.74\n",
            "[961 | 15822.99] loss=1.56 avg=1.74\n",
            "[962 | 15838.84] loss=1.77 avg=1.74\n",
            "[963 | 15854.56] loss=1.56 avg=1.74\n",
            "[964 | 15870.10] loss=1.95 avg=1.74\n",
            "[965 | 15885.49] loss=2.10 avg=1.74\n",
            "[966 | 15901.10] loss=1.55 avg=1.74\n",
            "[967 | 15916.91] loss=1.53 avg=1.74\n",
            "[968 | 15932.45] loss=1.54 avg=1.74\n",
            "[969 | 15947.98] loss=1.41 avg=1.73\n",
            "[970 | 15963.67] loss=1.91 avg=1.73\n",
            "[971 | 15979.93] loss=2.01 avg=1.74\n",
            "[972 | 15995.54] loss=1.60 avg=1.74\n",
            "[973 | 16011.53] loss=1.84 avg=1.74\n",
            "[974 | 16027.22] loss=2.29 avg=1.74\n",
            "[975 | 16043.00] loss=1.67 avg=1.74\n",
            "[976 | 16058.96] loss=1.78 avg=1.74\n",
            "[977 | 16074.75] loss=1.80 avg=1.74\n",
            "[978 | 16090.50] loss=1.98 avg=1.74\n",
            "[979 | 16106.20] loss=1.34 avg=1.74\n",
            "[980 | 16122.32] loss=1.44 avg=1.74\n",
            "[981 | 16138.74] loss=1.64 avg=1.74\n",
            "[982 | 16155.03] loss=1.57 avg=1.74\n",
            "[983 | 16171.12] loss=1.87 avg=1.74\n",
            "[984 | 16187.11] loss=1.87 avg=1.74\n",
            "[985 | 16203.22] loss=1.91 avg=1.74\n",
            "[986 | 16219.34] loss=1.65 avg=1.74\n",
            "[987 | 16235.58] loss=1.91 avg=1.74\n",
            "[988 | 16251.36] loss=1.82 avg=1.74\n",
            "[989 | 16266.89] loss=1.70 avg=1.74\n",
            "[990 | 16283.12] loss=1.66 avg=1.74\n",
            "[991 | 16298.91] loss=1.98 avg=1.74\n",
            "[992 | 16314.55] loss=1.70 avg=1.74\n",
            "[993 | 16330.22] loss=1.70 avg=1.74\n",
            "[994 | 16345.95] loss=1.62 avg=1.74\n",
            "[995 | 16361.63] loss=1.53 avg=1.74\n",
            "[996 | 16377.73] loss=1.95 avg=1.74\n",
            "[997 | 16393.57] loss=2.22 avg=1.75\n",
            "[998 | 16409.54] loss=1.45 avg=1.74\n",
            "[999 | 16425.21] loss=2.33 avg=1.75\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "00:00) the_ad: does any one know someone who can answer my question, which may be related to my HD? (this will most likely just be a question in the first place)\n",
            "(2009-07-18 07:01:00+00:00) the_ad: yes thank you\n",
            "(2009-07-18 07:02:00+00:00) the_ad: can someone please help?\n",
            "(2009-07-18 07:03:00+00:00) the_ad: i have another question, which i will ask as it is quite serious, and i would like help i have to help myself too\n",
            "(2009-07-18 07:05:00+00:00) the_ad: do you know how to get a working media player? and how?\n",
            "\n",
            "\n",
            "(2008-05-21 05:02:00+00:00) aaron_rhodes_: can you tell me the command to use to add the xorg.conf file to the desktop so it can be read by the nvidia drivers?    http://paste.ubuntu.com/231616/\n",
            "(2008-05-21 05:03:00+00:00) aaron_rhodes_: ok\n",
            "(2008-05-21 05:03:00+00:00) aaron_rhodes_: so i can use it now and delete this file in a better way?\n",
            "(2008-05-21 05:04:00+00:00) aaron_rhodes_: ok thanks\n",
            "(2008-05-21 05:05:00+00:00) aaron_rhodes_: can't you do it in xorg.conf?  i only have x11.5 and my drivers, so there'snt a file there?\n",
            "(2008-05-21 05:05:00+00:00) aaron_rhodes_: can't you do it in xorg.conf?\n",
            "(2008-05-21 05:05:00+00:00) aaron_rhodes_: or what?\n",
            "(2008-05-21 05:05:00+00:00) aaron_rhodes_: ok\n",
            "(2008-05-21 05:06:00+00:00) aaron_rhodes_: so would i delete this file first?\n",
            "(2008-05-21 05:06:00+00:00) aaron_rhodes_: ok i just changed the xorg.conf file on that cd, now i don't need to\n",
            "(2008-05-21 05:07:00+00:00) aaron_rhodes_: can't find 'xorg.conf' in my conf file, sorry for the confusion\n",
            "(2008-05-21 05:09:00+00:00) aaron_rhodes_: there is a file called xorg.conf , i only have the x11.4 driver installed, so there's no need to delete it first?\n",
            "(2008-05-21 05:09:00+00:00) aaron_rhodes_: but my xserver is xubuntu, and this has been installed, why couldn't i get nvidia?\n",
            "(2008-05-21 05:09:00+00:00) aaron_rhodes_:    i can't use my graphics card anymore, because of the incompatibility.\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: so if the file is xorg-conf but  you delete the one it has, will that work?\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: so how should i delete my files?\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: i can't use my graphics card anymore, because of the incompatibility\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: oh ok, ok, i deleted all my files and it is here... but how will I delete my xorg xsettings.conf file???\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: no other options?\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: ok im back, i can use the xorg files\n",
            "(2008-05-21 05:10:00+00:00) aaron_rhodes_: thanks for the help\n",
            "(2008-05-21\n",
            "\n",
            "[1000 | 16634.27] loss=1.50 avg=1.75\n",
            "[1001 | 16649.26] loss=1.87 avg=1.75\n",
            "[1002 | 16664.51] loss=1.96 avg=1.75\n",
            "[1003 | 16679.97] loss=1.71 avg=1.75\n",
            "[1004 | 16695.41] loss=1.93 avg=1.75\n",
            "[1005 | 16711.17] loss=2.22 avg=1.76\n",
            "[1006 | 16726.48] loss=1.79 avg=1.76\n",
            "[1007 | 16742.68] loss=1.72 avg=1.75\n",
            "[1008 | 16758.09] loss=1.69 avg=1.75\n",
            "[1009 | 16774.02] loss=2.01 avg=1.76\n",
            "[1010 | 16789.68] loss=1.66 avg=1.76\n",
            "[1011 | 16805.43] loss=1.85 avg=1.76\n",
            "[1012 | 16821.23] loss=1.25 avg=1.75\n",
            "[1013 | 16836.56] loss=2.06 avg=1.75\n",
            "[1014 | 16851.81] loss=1.39 avg=1.75\n",
            "[1015 | 16867.45] loss=1.84 avg=1.75\n",
            "[1016 | 16882.92] loss=1.42 avg=1.75\n",
            "[1017 | 16898.61] loss=1.62 avg=1.75\n",
            "[1018 | 16914.29] loss=1.26 avg=1.74\n",
            "[1019 | 16931.21] loss=1.93 avg=1.74\n",
            "[1020 | 16949.58] loss=1.93 avg=1.75\n",
            "[1021 | 16965.09] loss=1.98 avg=1.75\n",
            "[1022 | 16980.60] loss=2.11 avg=1.75\n",
            "[1023 | 16995.71] loss=1.98 avg=1.75\n",
            "[1024 | 17011.36] loss=1.83 avg=1.76\n",
            "[1025 | 17026.67] loss=1.33 avg=1.75\n",
            "[1026 | 17042.02] loss=1.84 avg=1.75\n",
            "[1027 | 17058.11] loss=2.05 avg=1.76\n",
            "[1028 | 17073.30] loss=2.16 avg=1.76\n",
            "[1029 | 17088.82] loss=1.68 avg=1.76\n",
            "[1030 | 17104.41] loss=1.59 avg=1.76\n",
            "[1031 | 17119.59] loss=1.79 avg=1.76\n",
            "[1032 | 17135.01] loss=2.02 avg=1.76\n",
            "[1033 | 17150.60] loss=1.64 avg=1.76\n",
            "[1034 | 17166.17] loss=2.04 avg=1.76\n",
            "[1035 | 17181.68] loss=1.83 avg=1.76\n",
            "[1036 | 17197.01] loss=1.76 avg=1.76\n",
            "[1037 | 17213.08] loss=1.33 avg=1.76\n",
            "[1038 | 17228.73] loss=1.78 avg=1.76\n",
            "[1039 | 17244.12] loss=1.81 avg=1.76\n",
            "[1040 | 17259.40] loss=1.76 avg=1.76\n",
            "[1041 | 17274.90] loss=2.02 avg=1.76\n",
            "[1042 | 17290.38] loss=1.55 avg=1.76\n",
            "[1043 | 17305.95] loss=1.92 avg=1.76\n",
            "[1044 | 17321.54] loss=1.40 avg=1.76\n",
            "[1045 | 17336.88] loss=1.80 avg=1.76\n",
            "[1046 | 17352.45] loss=1.55 avg=1.76\n",
            "[1047 | 17368.26] loss=1.84 avg=1.76\n",
            "[1048 | 17383.46] loss=1.98 avg=1.76\n",
            "[1049 | 17399.65] loss=2.10 avg=1.76\n",
            "[1050 | 17417.27] loss=1.74 avg=1.76\n",
            "[1051 | 17432.70] loss=1.76 avg=1.76\n",
            "[1052 | 17449.22] loss=1.41 avg=1.76\n",
            "[1053 | 17464.77] loss=1.43 avg=1.75\n",
            "[1054 | 17480.30] loss=1.82 avg=1.76\n",
            "[1055 | 17498.15] loss=1.28 avg=1.75\n",
            "[1056 | 17515.04] loss=1.90 avg=1.75\n",
            "[1057 | 17532.93] loss=1.97 avg=1.75\n",
            "[1058 | 17550.35] loss=1.96 avg=1.76\n",
            "[1059 | 17565.60] loss=1.33 avg=1.75\n",
            "[1060 | 17581.37] loss=1.39 avg=1.75\n",
            "[1061 | 17597.72] loss=1.77 avg=1.75\n",
            "[1062 | 17615.84] loss=1.53 avg=1.75\n",
            "[1063 | 17631.64] loss=1.56 avg=1.74\n",
            "[1064 | 17647.01] loss=1.73 avg=1.74\n",
            "[1065 | 17662.61] loss=1.70 avg=1.74\n",
            "[1066 | 17678.05] loss=2.19 avg=1.75\n",
            "[1067 | 17693.32] loss=1.51 avg=1.75\n",
            "[1068 | 17708.32] loss=1.64 avg=1.75\n",
            "[1069 | 17723.67] loss=1.61 avg=1.74\n",
            "[1070 | 17739.26] loss=1.68 avg=1.74\n",
            "[1071 | 17754.69] loss=1.51 avg=1.74\n",
            "[1072 | 17770.29] loss=1.69 avg=1.74\n",
            "[1073 | 17785.92] loss=1.61 avg=1.74\n",
            "[1074 | 17801.44] loss=1.64 avg=1.74\n",
            "[1075 | 17817.02] loss=1.71 avg=1.74\n",
            "[1076 | 17833.06] loss=1.47 avg=1.74\n",
            "[1077 | 17848.88] loss=1.71 avg=1.73\n",
            "[1078 | 17863.97] loss=1.46 avg=1.73\n",
            "[1079 | 17878.95] loss=1.42 avg=1.73\n",
            "[1080 | 17894.59] loss=1.64 avg=1.73\n",
            "[1081 | 17910.19] loss=1.97 avg=1.73\n",
            "[1082 | 17925.22] loss=1.74 avg=1.73\n",
            "[1083 | 17940.63] loss=2.22 avg=1.74\n",
            "[1084 | 17956.15] loss=2.11 avg=1.74\n",
            "[1085 | 17971.60] loss=1.80 avg=1.74\n",
            "[1086 | 17986.58] loss=2.20 avg=1.74\n",
            "[1087 | 18002.27] loss=1.88 avg=1.75\n",
            "[1088 | 18017.78] loss=1.58 avg=1.74\n",
            "[1089 | 18033.05] loss=1.74 avg=1.74\n",
            "[1090 | 18048.19] loss=2.19 avg=1.75\n",
            "[1091 | 18063.36] loss=1.96 avg=1.75\n",
            "[1092 | 18078.59] loss=1.99 avg=1.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nh3eOs5frWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fafff645-c18f-4e03-a853-58f99a7e875f"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --learning_rate 0.00001 --stop_after 3501 --model_name 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-12 04:34:18.584810: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-12 04:34:18.586377: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13f1800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-12 04:34:18.586415: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint checkpoint/run1/model-1501\n",
            "Loading dataset...\n",
            "100% 19/19 [00:30<00:00,  1.43s/it]\n",
            "dataset has 782061701 tokens\n",
            "Training...\n",
            "[1502 | 27.37] loss=1.64 avg=1.64\n",
            "[1503 | 46.27] loss=1.97 avg=1.80\n",
            "[1504 | 64.96] loss=1.21 avg=1.60\n",
            "[1505 | 84.22] loss=2.02 avg=1.71\n",
            "[1506 | 103.56] loss=1.66 avg=1.70\n",
            "[1507 | 122.82] loss=1.80 avg=1.72\n",
            "[1508 | 142.44] loss=1.81 avg=1.73\n",
            "[1509 | 162.63] loss=1.38 avg=1.69\n",
            "[1510 | 182.19] loss=1.95 avg=1.72\n",
            "[1511 | 202.49] loss=1.92 avg=1.74\n",
            "[1512 | 221.58] loss=1.57 avg=1.72\n",
            "[1513 | 241.04] loss=1.52 avg=1.70\n",
            "[1514 | 264.82] loss=1.38 avg=1.68\n",
            "[1515 | 284.07] loss=1.64 avg=1.67\n",
            "[1516 | 303.99] loss=1.69 avg=1.67\n",
            "[1517 | 323.42] loss=1.57 avg=1.67\n",
            "[1518 | 343.35] loss=1.51 avg=1.66\n",
            "[1519 | 362.71] loss=1.54 avg=1.65\n",
            "[1520 | 382.27] loss=1.39 avg=1.64\n",
            "[1521 | 402.08] loss=1.75 avg=1.64\n",
            "[1522 | 422.18] loss=2.16 avg=1.67\n",
            "[1523 | 442.12] loss=1.69 avg=1.67\n",
            "[1524 | 462.02] loss=1.64 avg=1.67\n",
            "[1525 | 482.45] loss=1.64 avg=1.67\n",
            "[1526 | 502.21] loss=1.82 avg=1.67\n",
            "[1527 | 522.22] loss=1.58 avg=1.67\n",
            "[1528 | 541.73] loss=1.61 avg=1.67\n",
            "[1529 | 560.89] loss=1.89 avg=1.68\n",
            "[1530 | 580.23] loss=1.69 avg=1.68\n",
            "[1531 | 599.40] loss=1.47 avg=1.67\n",
            "[1532 | 618.83] loss=1.62 avg=1.67\n",
            "[1533 | 638.71] loss=1.63 avg=1.67\n",
            "[1534 | 658.60] loss=1.64 avg=1.67\n",
            "[1535 | 678.93] loss=1.54 avg=1.66\n",
            "[1536 | 699.11] loss=1.47 avg=1.65\n",
            "[1537 | 718.55] loss=2.01 avg=1.67\n",
            "[1538 | 738.44] loss=1.48 avg=1.66\n",
            "[1539 | 758.60] loss=1.45 avg=1.65\n",
            "[1540 | 777.89] loss=1.94 avg=1.66\n",
            "[1541 | 798.73] loss=1.64 avg=1.66\n",
            "[1542 | 818.36] loss=1.83 avg=1.67\n",
            "[1543 | 838.20] loss=1.79 avg=1.67\n",
            "[1544 | 858.09] loss=1.51 avg=1.67\n",
            "[1545 | 877.67] loss=1.68 avg=1.67\n",
            "[1546 | 897.46] loss=1.69 avg=1.67\n",
            "[1547 | 917.30] loss=1.52 avg=1.66\n",
            "[1548 | 936.50] loss=1.65 avg=1.66\n",
            "[1549 | 956.18] loss=1.38 avg=1.65\n",
            "[1550 | 976.10] loss=1.65 avg=1.65\n",
            "[1551 | 995.57] loss=1.64 avg=1.65\n",
            "[1552 | 1014.96] loss=1.44 avg=1.65\n",
            "[1553 | 1034.72] loss=1.73 avg=1.65\n",
            "[1554 | 1054.15] loss=1.89 avg=1.66\n",
            "[1555 | 1073.51] loss=1.48 avg=1.65\n",
            "[1556 | 1093.02] loss=2.12 avg=1.66\n",
            "[1557 | 1114.27] loss=1.98 avg=1.67\n",
            "[1558 | 1134.10] loss=1.73 avg=1.67\n",
            "[1559 | 1153.38] loss=1.94 avg=1.68\n",
            "[1560 | 1173.11] loss=1.65 avg=1.68\n",
            "[1561 | 1193.54] loss=1.85 avg=1.68\n",
            "[1562 | 1213.52] loss=1.84 avg=1.69\n",
            "[1563 | 1233.51] loss=1.37 avg=1.68\n",
            "[1564 | 1252.93] loss=1.70 avg=1.68\n",
            "[1565 | 1272.37] loss=1.63 avg=1.68\n",
            "[1566 | 1292.22] loss=2.27 avg=1.69\n",
            "[1567 | 1312.10] loss=2.07 avg=1.70\n",
            "[1568 | 1332.17] loss=1.52 avg=1.69\n",
            "[1569 | 1351.38] loss=1.73 avg=1.70\n",
            "[1570 | 1371.07] loss=1.53 avg=1.69\n",
            "[1571 | 1391.03] loss=1.41 avg=1.69\n",
            "[1572 | 1410.61] loss=1.29 avg=1.68\n",
            "[1573 | 1430.49] loss=1.84 avg=1.68\n",
            "[1574 | 1450.49] loss=2.14 avg=1.69\n",
            "[1575 | 1470.38] loss=1.98 avg=1.70\n",
            "[1576 | 1490.40] loss=1.84 avg=1.70\n",
            "[1577 | 1509.98] loss=1.52 avg=1.70\n",
            "[1578 | 1529.65] loss=1.75 avg=1.70\n",
            "[1579 | 1549.24] loss=2.16 avg=1.70\n",
            "[1580 | 1568.95] loss=1.96 avg=1.71\n",
            "[1581 | 1589.04] loss=1.14 avg=1.70\n",
            "[1582 | 1608.19] loss=1.48 avg=1.70\n",
            "[1583 | 1627.74] loss=1.58 avg=1.69\n",
            "[1584 | 1647.03] loss=1.60 avg=1.69\n",
            "[1585 | 1666.74] loss=1.62 avg=1.69\n",
            "[1586 | 1686.01] loss=1.41 avg=1.69\n",
            "[1587 | 1706.03] loss=1.68 avg=1.69\n",
            "[1588 | 1727.01] loss=1.67 avg=1.69\n",
            "[1589 | 1746.84] loss=1.62 avg=1.68\n",
            "[1590 | 1766.00] loss=1.30 avg=1.68\n",
            "[1591 | 1785.55] loss=1.50 avg=1.67\n",
            "[1592 | 1805.28] loss=1.70 avg=1.67\n",
            "[1593 | 1824.32] loss=1.93 avg=1.68\n",
            "[1594 | 1843.74] loss=1.49 avg=1.68\n",
            "[1595 | 1863.10] loss=1.73 avg=1.68\n",
            "[1596 | 1882.62] loss=1.61 avg=1.68\n",
            "[1597 | 1902.53] loss=1.57 avg=1.67\n",
            "[1598 | 1922.50] loss=1.52 avg=1.67\n",
            "[1599 | 1941.96] loss=1.61 avg=1.67\n",
            "[1600 | 1962.43] loss=1.85 avg=1.67\n",
            "[1601 | 1982.75] loss=1.87 avg=1.68\n",
            "[1602 | 2002.20] loss=2.08 avg=1.68\n",
            "[1603 | 2022.03] loss=1.60 avg=1.68\n",
            "[1604 | 2042.47] loss=1.93 avg=1.69\n",
            "[1605 | 2062.48] loss=1.22 avg=1.68\n",
            "[1606 | 2082.32] loss=1.90 avg=1.68\n",
            "[1607 | 2102.10] loss=1.77 avg=1.68\n",
            "[1608 | 2121.80] loss=1.89 avg=1.69\n",
            "[1609 | 2141.65] loss=1.80 avg=1.69\n",
            "[1610 | 2161.58] loss=1.95 avg=1.69\n",
            "[1611 | 2181.40] loss=1.53 avg=1.69\n",
            "[1612 | 2201.06] loss=1.59 avg=1.69\n",
            "[1613 | 2220.36] loss=2.12 avg=1.69\n",
            "[1614 | 2239.84] loss=1.62 avg=1.69\n",
            "[1615 | 2259.10] loss=1.50 avg=1.69\n",
            "[1616 | 2278.51] loss=1.41 avg=1.69\n",
            "[1617 | 2298.55] loss=1.50 avg=1.68\n",
            "[1618 | 2318.47] loss=2.09 avg=1.69\n",
            "[1619 | 2339.07] loss=2.00 avg=1.69\n",
            "[1620 | 2359.71] loss=1.86 avg=1.70\n",
            "[1621 | 2379.27] loss=1.81 avg=1.70\n",
            "[1622 | 2399.19] loss=2.13 avg=1.70\n",
            "[1623 | 2418.82] loss=1.93 avg=1.71\n",
            "[1624 | 2438.89] loss=1.56 avg=1.71\n",
            "[1625 | 2458.77] loss=1.62 avg=1.70\n",
            "[1626 | 2477.72] loss=1.80 avg=1.71\n",
            "[1627 | 2497.30] loss=1.80 avg=1.71\n",
            "[1628 | 2516.64] loss=2.28 avg=1.71\n",
            "[1629 | 2535.67] loss=1.53 avg=1.71\n",
            "[1630 | 2555.36] loss=1.83 avg=1.71\n",
            "[1631 | 2574.46] loss=1.90 avg=1.72\n",
            "[1632 | 2594.05] loss=1.51 avg=1.71\n",
            "[1633 | 2613.83] loss=1.55 avg=1.71\n",
            "[1634 | 2634.13] loss=1.68 avg=1.71\n",
            "[1635 | 2654.44] loss=1.75 avg=1.71\n",
            "[1636 | 2674.96] loss=1.65 avg=1.71\n",
            "[1637 | 2694.18] loss=2.18 avg=1.72\n",
            "[1638 | 2713.36] loss=1.65 avg=1.72\n",
            "[1639 | 2732.75] loss=1.62 avg=1.71\n",
            "[1640 | 2752.06] loss=1.75 avg=1.71\n",
            "[1641 | 2772.41] loss=1.65 avg=1.71\n",
            "[1642 | 2792.36] loss=1.86 avg=1.72\n",
            "[1643 | 2811.91] loss=1.66 avg=1.72\n",
            "[1644 | 2831.40] loss=1.53 avg=1.71\n",
            "[1645 | 2850.67] loss=1.78 avg=1.71\n",
            "[1646 | 2869.99] loss=1.74 avg=1.71\n",
            "[1647 | 2889.68] loss=1.61 avg=1.71\n",
            "[1648 | 2908.68] loss=1.89 avg=1.71\n",
            "[1649 | 2928.25] loss=2.04 avg=1.72\n",
            "[1650 | 2947.85] loss=1.36 avg=1.71\n",
            "[1651 | 2968.25] loss=1.92 avg=1.72\n",
            "[1652 | 2987.58] loss=1.91 avg=1.72\n",
            "[1653 | 3007.50] loss=1.63 avg=1.72\n",
            "[1654 | 3026.67] loss=1.64 avg=1.72\n",
            "[1655 | 3045.93] loss=1.68 avg=1.72\n",
            "[1656 | 3065.98] loss=1.73 avg=1.72\n",
            "[1657 | 3085.92] loss=1.70 avg=1.72\n",
            "[1658 | 3105.36] loss=1.59 avg=1.72\n",
            "[1659 | 3125.65] loss=1.38 avg=1.71\n",
            "[1660 | 3145.24] loss=1.62 avg=1.71\n",
            "[1661 | 3165.02] loss=1.17 avg=1.70\n",
            "[1662 | 3184.71] loss=2.35 avg=1.71\n",
            "[1663 | 3204.31] loss=1.65 avg=1.71\n",
            "[1664 | 3223.89] loss=1.46 avg=1.71\n",
            "[1665 | 3243.44] loss=1.97 avg=1.71\n",
            "[1666 | 3263.34] loss=1.77 avg=1.71\n",
            "[1667 | 3283.80] loss=1.62 avg=1.71\n",
            "[1668 | 3302.84] loss=2.25 avg=1.72\n",
            "[1669 | 3322.61] loss=1.87 avg=1.72\n",
            "[1670 | 3342.51] loss=1.52 avg=1.72\n",
            "[1671 | 3362.22] loss=1.47 avg=1.71\n",
            "[1672 | 3382.01] loss=1.99 avg=1.72\n",
            "[1673 | 3401.32] loss=1.65 avg=1.72\n",
            "[1674 | 3421.12] loss=1.63 avg=1.71\n",
            "[1675 | 3441.11] loss=1.77 avg=1.72\n",
            "[1676 | 3460.92] loss=1.96 avg=1.72\n",
            "[1677 | 3481.09] loss=1.69 avg=1.72\n",
            "[1678 | 3501.20] loss=1.79 avg=1.72\n",
            "[1679 | 3521.29] loss=1.63 avg=1.72\n",
            "[1680 | 3540.54] loss=1.87 avg=1.72\n",
            "[1681 | 3560.79] loss=1.65 avg=1.72\n",
            "[1682 | 3580.43] loss=1.68 avg=1.72\n",
            "[1683 | 3601.41] loss=1.69 avg=1.72\n",
            "[1684 | 3621.25] loss=1.83 avg=1.72\n",
            "[1685 | 3641.07] loss=1.01 avg=1.71\n",
            "[1686 | 3661.14] loss=1.72 avg=1.71\n",
            "[1687 | 3680.21] loss=1.45 avg=1.71\n",
            "[1688 | 3699.83] loss=1.64 avg=1.71\n",
            "[1689 | 3719.72] loss=1.87 avg=1.71\n",
            "[1690 | 3739.09] loss=1.66 avg=1.71\n",
            "[1691 | 3759.06] loss=2.11 avg=1.71\n",
            "[1692 | 3779.27] loss=1.87 avg=1.71\n",
            "[1693 | 3798.86] loss=1.18 avg=1.71\n",
            "[1694 | 3818.52] loss=1.68 avg=1.71\n",
            "[1695 | 3838.11] loss=1.60 avg=1.71\n",
            "[1696 | 3858.55] loss=1.51 avg=1.70\n",
            "[1697 | 3878.98] loss=1.72 avg=1.71\n",
            "[1698 | 3898.87] loss=1.43 avg=1.70\n",
            "[1699 | 3918.84] loss=1.46 avg=1.70\n",
            "[1700 | 3938.91] loss=1.49 avg=1.70\n",
            "[1701 | 3958.65] loss=1.37 avg=1.69\n",
            "[1702 | 3978.72] loss=1.55 avg=1.69\n",
            "[1703 | 3998.41] loss=1.89 avg=1.69\n",
            "[1704 | 4018.47] loss=1.35 avg=1.69\n",
            "[1705 | 4038.42] loss=1.38 avg=1.69\n",
            "[1706 | 4058.15] loss=1.56 avg=1.68\n",
            "[1707 | 4078.12] loss=1.70 avg=1.68\n",
            "[1708 | 4098.00] loss=1.48 avg=1.68\n",
            "[1709 | 4117.78] loss=1.70 avg=1.68\n",
            "[1710 | 4137.32] loss=1.64 avg=1.68\n",
            "[1711 | 4157.05] loss=1.49 avg=1.68\n",
            "[1712 | 4177.14] loss=1.39 avg=1.68\n",
            "[1713 | 4196.92] loss=1.63 avg=1.68\n",
            "[1714 | 4217.48] loss=2.04 avg=1.68\n",
            "[1715 | 4237.04] loss=1.67 avg=1.68\n",
            "[1716 | 4256.44] loss=0.89 avg=1.67\n",
            "[1717 | 4276.45] loss=1.47 avg=1.67\n",
            "[1718 | 4296.35] loss=1.56 avg=1.67\n",
            "[1719 | 4316.66] loss=2.12 avg=1.67\n",
            "[1720 | 4336.59] loss=2.41 avg=1.68\n",
            "[1721 | 4356.55] loss=1.61 avg=1.68\n",
            "[1722 | 4376.49] loss=1.85 avg=1.68\n",
            "[1723 | 4396.35] loss=1.87 avg=1.68\n",
            "[1724 | 4416.18] loss=1.21 avg=1.68\n",
            "[1725 | 4436.23] loss=1.46 avg=1.68\n",
            "[1726 | 4455.76] loss=1.57 avg=1.68\n",
            "[1727 | 4476.35] loss=2.27 avg=1.68\n",
            "[1728 | 4496.23] loss=1.50 avg=1.68\n",
            "[1729 | 4516.43] loss=1.62 avg=1.68\n",
            "[1730 | 4537.22] loss=1.65 avg=1.68\n",
            "[1731 | 4556.97] loss=1.71 avg=1.68\n",
            "[1732 | 4576.59] loss=1.58 avg=1.68\n",
            "[1733 | 4597.47] loss=1.97 avg=1.68\n",
            "[1734 | 4617.12] loss=1.61 avg=1.68\n",
            "[1735 | 4636.86] loss=1.92 avg=1.68\n",
            "[1736 | 4656.57] loss=1.29 avg=1.68\n",
            "[1737 | 4676.17] loss=1.68 avg=1.68\n",
            "[1738 | 4695.89] loss=1.77 avg=1.68\n",
            "[1739 | 4715.73] loss=1.81 avg=1.68\n",
            "[1740 | 4735.05] loss=1.37 avg=1.68\n",
            "[1741 | 4754.79] loss=1.93 avg=1.68\n",
            "[1742 | 4774.37] loss=1.68 avg=1.68\n",
            "[1743 | 4794.16] loss=1.48 avg=1.68\n",
            "[1744 | 4814.02] loss=2.06 avg=1.68\n",
            "[1745 | 4835.56] loss=1.35 avg=1.68\n",
            "[1746 | 4855.63] loss=1.55 avg=1.68\n",
            "[1747 | 4874.87] loss=1.77 avg=1.68\n",
            "[1748 | 4893.76] loss=1.43 avg=1.68\n",
            "[1749 | 4913.28] loss=1.41 avg=1.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "-14-20 05:54:00+00:00) almox: thanks for your help.\n",
            "(2009-04-14 05:55:00+00:00) almox: yes i believe so\n",
            "(2009-04-14 05:56:00+00:00) almox: did you just do it from a shell or what?\n",
            "(2009-04-14 05:56:00+00:00) almox: i know\n",
            "(2009-04-14 05:56:00+00:00) almox: that's a pain in the neck lol\n",
            "(2009-04-14 05:57:00+00:00) jimbe: are you using kde? or ubuntu?\n",
            "(2009-04-14 05:58:00+00:00) almox: kde... i know i haven't been online for days.. but that's what i am using right now\n",
            "(2009-04-14 05:58:00+00:00) jimbe: then you have to change /etc/apt/sources.list.d/* to the contents of /etc/apt/sources.list.d/repo-2.6.11_ubuntu-desktop-i386\n",
            "(2009-04-14 05:59:00+00:00) almox: and i was to change only a single line?\n",
            "(2009-04-14 05:59:00+00:00) jimbe: thats odd.\n",
            "(2009-04-14 06:02:00+00:00) almox: im trying to change everything but that one line\n",
            "(2009-04-14 06:03:00+00:00) jimbe: that's what i think you are doing.  then i recommend to reinstall the package, and then you can do a fresh install\n",
            "\n",
            "\n",
            "(2008-08-25 23:53:00+00:00) nathalie: How do I find out how many of the installed packages are installed at once?\n",
            "(2008-08-25 23:54:00+00:00) nathalie: Thanks,\n",
            "(2008-08-25 23:55:00+00:00) nathalie: And how do I do that?\n",
            "(2008-08-25 23:55:00+00:00) nathalie: I can only view the files in the /var dir, only one package is installed at a time?\n",
            "(2008-08-25 23:55:00+00:00) nathalie: And then what?\n",
            "(2008-08-25 23:56:00+00:00) nathalie: What would be the best way to get to these files if I were to run 'ls -lat > /tmp'?\n",
            "(2008-08-25 23:56:00+00:00) nathalie: That's what i was using before\n",
            "(2008-08-25 23:57:00+00:00) nathalie: I have one for every package\n",
            "(2008-08-25 23:58:00+00:00) nathalie: Ok, then I'm trying to get to that folder at a time, will there be a problem?\n",
            "(2008-08-25 23:59:00+00:00) nathalie: Thanks.\n",
            "(2008-08-26 00:01:00+00:00) nathalie: Did you do some other kind of test, like the 'apt-get update' thing?\n",
            "(2008-08-26 00:01:00+00:00) nathalie: Oh! Then just do 'sudo apt-get update'\n",
            "(2008-08-26 00:01:00+00:00) nathalie: I guess I could try it after the update fails, but not sure.\n",
            "(2008-08-26 00:01:00+00:00) nathalie: Anyhoo, thanks again, it helped.\n",
            "(2008-08-26 00:02:00+00:00) nathalie: I'll install them now, now if it fails I might do a fresh install or do it from an external, no worries though.\n",
            "\n",
            "\n",
            "(2010-10-08 00:20:00+00:00) davey:  so can't have a live CD? I tried booting from a live cd but with the live cd on it\n",
            "(2010-10-08 00:21:00+00:00) davey:  im using ubuntu 10.04 that had a bug or something when it didnt know its booting from the SD\n",
            "\n",
            "\n",
            "[1750 | 5183.68] loss=1.67 avg=1.67\n",
            "[1751 | 5204.17] loss=2.02 avg=1.68\n",
            "[1752 | 5224.12] loss=1.43 avg=1.67\n",
            "[1753 | 5243.29] loss=1.69 avg=1.67\n",
            "[1754 | 5263.16] loss=1.74 avg=1.68\n",
            "[1755 | 5284.24] loss=1.59 avg=1.67\n",
            "[1756 | 5305.07] loss=1.58 avg=1.67\n",
            "[1757 | 5324.80] loss=1.89 avg=1.68\n",
            "[1758 | 5341.51] loss=1.40 avg=1.67\n",
            "[1759 | 5361.46] loss=1.42 avg=1.67\n",
            "[1760 | 5381.61] loss=1.52 avg=1.67\n",
            "[1761 | 5402.15] loss=2.02 avg=1.67\n",
            "[1762 | 5422.27] loss=1.62 avg=1.67\n",
            "[1763 | 5443.04] loss=1.74 avg=1.67\n",
            "[1764 | 5464.18] loss=1.86 avg=1.67\n",
            "[1765 | 5484.66] loss=1.83 avg=1.68\n",
            "[1766 | 5505.07] loss=1.67 avg=1.68\n",
            "[1767 | 5525.18] loss=1.61 avg=1.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW_GO8WmFCHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --learning_rate 0.000001 --stop_after 5001 --model_name 345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o34StYeYMw_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.95 --model_name 345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ytAkrYNHZjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}